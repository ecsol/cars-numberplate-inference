#!/usr/bin/env python3
"""
è»Šä¸¡ç”»åƒã‚’å–å¾—ã—ã€ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ãƒãƒƒãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. DBã‹ã‚‰æŒ‡å®šæ—¥ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã‚’å–å¾—
2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã§å‡¦ç†æ¸ˆã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
3. å„è»Šä¸¡ã®æœ€åˆã®ç”»åƒ(branch_no=1): ãƒã‚¹ã‚¯ + ãƒãƒŠãƒ¼è¿½åŠ 
4. ãã®ä»–ã®ç”»åƒ: ãƒã‚¹ã‚¯ã®ã¿
5. å…ƒç”»åƒã‚’.backupãƒ•ã‚©ãƒ«ãƒ€ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
6. å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«è¨˜éŒ²

Usage:
    python fetch_today_images.py                    # ä»Šæ—¥ã®ç”»åƒã€æœ€å¤§10ä»¶
    python fetch_today_images.py --days-ago 1      # æ˜¨æ—¥ã®ç”»åƒ
    python fetch_today_images.py --limit 50        # æœ€å¤§50ä»¶å‡¦ç†
    python fetch_today_images.py --days-ago 7 --limit 100

crontab: * * * * * /path/to/venv/bin/python /path/to/fetch_today_images.py
"""

import argparse
import json
import os
import sys
import shutil
import warnings
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional

# Suppress boto3 Python 3.9 deprecation warning
warnings.filterwarnings("ignore", message=".*Boto3 will no longer support Python 3.9.*")

import boto3
from botocore.exceptions import ClientError
import psycopg2
import requests

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
SCRIPT_DIR = Path(__file__).parent
PROJECT_DIR = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_DIR))


def load_env_file():
    """ç’°å¢ƒå¤‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    env_file = PROJECT_DIR / ".env"
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    if key not in os.environ:
                        os.environ[key] = value


load_env_file()

from scripts.process_image_v2 import process_image

# ======================
# è¨­å®š
# ======================
DB_CONFIG = {
    "host": os.getenv("DB_HOST", ""),
    "database": os.getenv("DB_NAME", "cartrading"),
    "user": os.getenv("DB_USER", ""),
    "password": os.getenv("DB_PASSWORD", ""),
}

S3_MOUNT = os.getenv("S3_MOUNT", "")

# Two-Stage ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
SEG_MODEL_PATH = os.getenv(
    "SEG_MODEL_PATH", str(PROJECT_DIR / "models" / "best_yolo26x_lambda_20260201.pt")
)
POSE_MODEL_PATH = os.getenv(
    "POSE_MODEL_PATH", str(PROJECT_DIR / "models" / "yolo26x_pose_best.pt")
)
PLATE_MASK_PATH = os.getenv(
    "PLATE_MASK_PATH", str(PROJECT_DIR / "assets" / "plate_mask.png")
)

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€å†… logs/ï¼‰
_log_dir_env = os.getenv("LOG_DIR", "")
if _log_dir_env:
    LOG_DIR = Path(_log_dir_env)
    if not LOG_DIR.is_absolute():
        LOG_DIR = PROJECT_DIR / _log_dir_env
else:
    LOG_DIR = PROJECT_DIR / "logs"
LOG_FILE = LOG_DIR / "process.log"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
# BACKUP_S3_BUCKET: boto3ã§S3ã«ç›´æ¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ - mountpoint-s3ã®åˆ¶é™ã‚’å›é¿ï¼‰
# BACKUP_DIR: ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
# ã©ã¡ã‚‰ã‚‚æœªè¨­å®š: ã‚¨ãƒ©ãƒ¼
BACKUP_S3_BUCKET = os.getenv("BACKUP_S3_BUCKET", "")  # ä¾‹: cs1es3
BACKUP_S3_PREFIX = os.getenv("BACKUP_S3_PREFIX", ".backup")  # S3 key prefix
BACKUP_DIR = os.getenv("BACKUP_DIR", "")

# boto3 S3 client (lazy init)
_s3_client = None

# Chatworké€šçŸ¥è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
CHATWORK_API_KEY = os.getenv("CHATWORK_API_KEY", "")
CHATWORK_ROOM_ID = os.getenv("CHATWORK_ROOM_ID", "")
# ç”»åƒã®ãƒ™ãƒ¼ã‚¹URLï¼ˆChatworké€šçŸ¥ç”¨ï¼‰
IMAGE_BASE_URL = os.getenv("IMAGE_BASE_URL", "https://www.autobacs-cars-system.com")


def get_s3_client():
    """S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰"""
    global _s3_client
    if _s3_client is None:
        _s3_client = boto3.client("s3")
    return _s3_client


def send_chatwork_notification(message: str) -> bool:
    """
    Chatworkã«é€šçŸ¥ã‚’é€ä¿¡

    Args:
        message: é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        bool: é€ä¿¡æˆåŠŸã‹ã©ã†ã‹
    """
    if not CHATWORK_API_KEY or not CHATWORK_ROOM_ID:
        return False

    try:
        url = f"https://api.chatwork.com/v2/rooms/{CHATWORK_ROOM_ID}/messages"
        headers = {"X-ChatWorkToken": CHATWORK_API_KEY}
        data = {"body": message}

        response = requests.post(url, headers=headers, data=data, timeout=10)
        response.raise_for_status()
        return True
    except Exception as e:
        logger.error(f"Chatworké€šçŸ¥å¤±æ•—: {e}")
        return False


def build_processing_summary(
    target_date: datetime.date,
    stats: dict,
    car_results: list,
) -> str:
    """
    å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ

    Args:
        target_date: å¯¾è±¡æ—¥
        stats: çµ±è¨ˆæƒ…å ±
        car_results: è»Šä¸¡ã”ã¨ã®å‡¦ç†çµæœ
            [(car_id, success_count, error_count, detections, first_image_path), ...]

    Returns:
        str: Chatworkç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    lines = [
        "[info][title]ğŸš— ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆå‡¦ç†å®Œäº†[/title]",
        f"ğŸ“… å¯¾è±¡æ—¥: {target_date}",
        f"âœ… æˆåŠŸ: {stats['success']}ä»¶",
        f"âŒ ã‚¨ãƒ©ãƒ¼: {stats['error']}ä»¶",
        f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {stats['skip_tracked'] + stats['skip_other']}ä»¶",
        "[/info]",
        "",
    ]

    if car_results:
        lines.append("[info][title]ğŸ“Š è»Šä¸¡åˆ¥çµæœ[/title]")
        for car_id, success, error, detections, first_image_path in car_results[:10]:
            status_icon = "âœ…" if error == 0 else "âš ï¸"
            lines.append(
                f"{status_icon} {car_id}: {success}æšå‡¦ç†, æ¤œå‡º{detections}ä»¶"
            )
            # æœ€åˆã®ç”»åƒã®URLã‚’è¿½åŠ 
            if first_image_path:
                image_url = f"{IMAGE_BASE_URL}{first_image_path}"
                lines.append(f"   {image_url}")
            lines.append("")

        if len(car_results) > 10:
            lines.append(f"... ä»– {len(car_results) - 10}å°")
        lines.append("[/info]")

    return "\n".join(lines)


def s3_backup_exists(s3_key: str) -> bool:
    """S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
    try:
        get_s3_client().head_object(Bucket=BACKUP_S3_BUCKET, Key=s3_key)
        return True
    except ClientError as e:
        if e.response["Error"]["Code"] == "404":
            return False
        raise


def s3_upload_backup(local_path: str, s3_key: str):
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    get_s3_client().upload_file(local_path, BACKUP_S3_BUCKET, s3_key)


def s3_download_backup(s3_key: str, local_path: str):
    """S3ã‹ã‚‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆmountpoint-s3å¯¾å¿œï¼‰"""
    # mountpoint-s3ã¯shutil.copyãŒå‹•ã‹ãªã„ãŸã‚ã€
    # ãƒã‚¤ãƒˆå˜ä½ã§ç›´æ¥æ›¸ãè¾¼ã‚€
    response = get_s3_client().get_object(Bucket=BACKUP_S3_BUCKET, Key=s3_key)
    data = response["Body"].read()

    with open(local_path, "wb") as f:
        f.write(data)


# ======================
# ãƒ­ã‚®ãƒ³ã‚°
# ======================
class Logger:
    """è©³ç´°ãƒ­ã‚°å‡ºåŠ›ã‚¯ãƒ©ã‚¹"""

    def __init__(self, log_file: Path):
        self.log_file = log_file
        try:
            self.log_file.parent.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            self.log_file = Path("./process.log")
            print(f"[WARN] ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆä¸å¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {self.log_file}")

    def _write(self, level: str, message: str):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        log_line = f"[{timestamp}] [{level}] {message}"
        print(log_line)

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(log_line + "\n")
        except Exception:
            pass

    def info(self, message: str):
        self._write("INFO", message)

    def error(self, message: str):
        self._write("ERROR", message)

    def warn(self, message: str):
        self._write("WARN", message)

    def debug(self, message: str):
        self._write("DEBUG", message)

    def success(self, message: str):
        self._write("OK", message)


logger = Logger(LOG_FILE)

# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
LOG_RETENTION_DAYS = int(os.getenv("LOG_RETENTION_DAYS", "60"))


# ======================
# ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
# ======================
class ProcessingTracker:
    """
    å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°

    æ—¥ä»˜ã”ã¨ã«JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†:
    {PROJECT_DIR}/logs/tracking/processed_20260130.json

    å½¢å¼:
    {
        "date": "2026-01-30",
        "processed": {
            "12345": {
                "file_id": 12345,
                "path": "/upfile/1007/4856/xxx.jpg",
                "processed_at": "2026-01-30 12:34:56",
                "status": "success",
                "detections": 1,
                "is_first": true
            },
            ...
        }
    }
    """

    def __init__(self, log_dir: Path):
        self.tracking_dir = log_dir / "tracking"
        try:
            self.tracking_dir.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            self.tracking_dir = Path("./tracking")
            self.tracking_dir.mkdir(parents=True, exist_ok=True)

    def _get_tracking_file(self, target_date: datetime.date) -> Path:
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.tracking_dir / f"processed_{target_date.strftime('%Y%m%d')}.json"

    def load(self, target_date: datetime.date) -> dict:
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        tracking_file = self._get_tracking_file(target_date)

        if not tracking_file.exists():
            return {
                "date": target_date.isoformat(),
                "created_at": datetime.now().isoformat(),
                "processed": {},
            }

        try:
            with open(tracking_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return {
                "date": target_date.isoformat(),
                "created_at": datetime.now().isoformat(),
                "processed": {},
            }

    def save(self, target_date: datetime.date, data: dict):
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        tracking_file = self._get_tracking_file(target_date)
        data["updated_at"] = datetime.now().isoformat()

        try:
            with open(tracking_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {e}")

    def is_processed(self, target_date: datetime.date, file_id: int) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹"""
        data = self.load(target_date)
        return str(file_id) in data["processed"]

    def has_car_any_processed(
        self, target_date: datetime.date, car_path_prefix: str
    ) -> bool:
        """è»Šä¸¡ã®ã„ãšã‚Œã‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ï¼ˆãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒã‚§ãƒƒã‚¯ï¼‰

        Args:
            car_path_prefix: ä¾‹ "/upfile/1041/8430/"
        """
        data = self.load(target_date)
        for record in data.get("processed", {}).values():
            path = record.get("path", "")
            if path.startswith(car_path_prefix):
                return True
        return False

    def mark_processed(
        self,
        target_date: datetime.date,
        file_id: int,
        path: str,
        status: str,
        detections: int = 0,
        is_first: bool = False,
        branch_no: Optional[int] = None,
        error_reason: Optional[str] = None,
    ):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        data = self.load(target_date)

        record = {
            "file_id": file_id,
            "path": path,
            "branch_no": branch_no,
            "processed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "status": status,
            "detections": detections,
            "is_first": is_first,
        }

        if error_reason:
            record["error"] = error_reason

        data["processed"][str(file_id)] = record
        self.save(target_date, data)

    def get_stats(self, target_date: datetime.date) -> dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        data = self.load(target_date)
        processed = data.get("processed", {})

        stats = {
            "total": len(processed),
            "success": 0,
            "error": 0,
            "skip": 0,
        }

        for record in processed.values():
            status = record.get("status", "unknown")
            if status in stats:
                stats[status] += 1

        return stats

    def get_last_processed_time(self, target_date: datetime.date) -> Optional[datetime]:
        """æœ€å¾Œã«å‡¦ç†ã—ãŸæ™‚åˆ»ã‚’å–å¾—"""
        data = self.load(target_date)
        last_time_str = data.get("last_processed_time")
        if last_time_str:
            try:
                return datetime.fromisoformat(last_time_str)
            except ValueError:
                return None
        return None

    def set_last_processed_time(self, target_date: datetime.date, last_time: datetime):
        """æœ€å¾Œã«å‡¦ç†ã—ãŸæ™‚åˆ»ã‚’ä¿å­˜"""
        data = self.load(target_date)
        data["last_processed_time"] = last_time.isoformat()
        self.save(target_date, data)

    def cleanup_old_files(self, retention_days: int = 60):
        """å¤ã„ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        if not self.tracking_dir.exists():
            return

        cutoff_date = datetime.now().date() - timedelta(days=retention_days)
        deleted_count = 0

        for file_path in self.tracking_dir.glob("processed_*.json"):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º: processed_20260130.json
                date_str = file_path.stem.replace("processed_", "")
                file_date = datetime.strptime(date_str, "%Y%m%d").date()

                if file_date < cutoff_date:
                    file_path.unlink()
                    deleted_count += 1
                    logger.debug(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
            except (ValueError, OSError) as e:
                logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {file_path.name} - {e}")

        if deleted_count > 0:
            logger.info(
                f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {deleted_count}ä»¶å‰Šé™¤ ({retention_days}æ—¥ä»¥å‰)"
            )


tracker = ProcessingTracker(LOG_DIR)


# ======================
# ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒã‚¹ã‚¯èª­ã¿è¾¼ã¿ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ« - èµ·å‹•æ™‚ã«1å›ã®ã¿ï¼‰
# ======================
seg_model = None
pose_model = None
mask_image = None


def load_models():
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒã‚¹ã‚¯ç”»åƒã‚’èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã®ã¿ï¼‰"""
    global seg_model, pose_model, mask_image

    if seg_model is None:
        from ultralytics import YOLO

        logger.info(f"Segãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {SEG_MODEL_PATH}")
        seg_model = YOLO(SEG_MODEL_PATH)

    if pose_model is None:
        from ultralytics import YOLO

        logger.info(f"Poseãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {POSE_MODEL_PATH}")
        pose_model = YOLO(POSE_MODEL_PATH)

    if mask_image is None:
        import cv2

        logger.info(f"ãƒã‚¹ã‚¯ç”»åƒèª­ã¿è¾¼ã¿: {PLATE_MASK_PATH}")
        mask_image = cv2.imread(PLATE_MASK_PATH, cv2.IMREAD_UNCHANGED)
        if mask_image is None:
            raise ValueError(f"ãƒã‚¹ã‚¯ç”»åƒã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {PLATE_MASK_PATH}")

    return seg_model, pose_model, mask_image


# ======================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
# ======================
def get_images_by_date(
    target_date: datetime.date, last_fetch_time: Optional[datetime] = None
) -> list:
    """
    æŒ‡å®šæ—¥ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã‚’å–å¾—

    Args:
        target_date: å¯¾è±¡æ—¥
        last_fetch_time: ã“ã®æ™‚åˆ»ä»¥é™ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã®ã¿å–å¾—ï¼ˆå¢—åˆ†å–å¾—ã§DBè² è·è»½æ¸›ï¼‰

    Returns:
        list: [(id, car_cd, inspresultdata_cd, branch_no, save_file_name, created, modified), ...]
    """

    if last_fetch_time:
        # å¢—åˆ†å–å¾—: last_fetch_timeä»¥é™ã®æ–°è¦/æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
        query = """
            SELECT 
                id,
                car_cd,
                inspresultdata_cd,
                branch_no,
                save_file_name,
                created,
                modified
            FROM upload_files
            WHERE (DATE(created) = %s OR DATE(modified) = %s)
              AND (created > %s OR modified > %s)
              AND delete_flg = 0
              AND save_file_name IS NOT NULL
              AND save_file_name != ''
            ORDER BY 
                COALESCE(inspresultdata_cd, car_cd::text),
                branch_no ASC
        """
        params = (target_date, target_date, last_fetch_time, last_fetch_time)
    else:
        # åˆå›: å…¨ä»¶å–å¾—
        query = """
            SELECT 
                id,
                car_cd,
                inspresultdata_cd,
                branch_no,
                save_file_name,
                created,
                modified
            FROM upload_files
            WHERE (DATE(created) = %s OR DATE(modified) = %s)
              AND delete_flg = 0
              AND save_file_name IS NOT NULL
              AND save_file_name != ''
            ORDER BY 
                COALESCE(inspresultdata_cd, car_cd::text),
                branch_no ASC
        """
        params = (target_date, target_date)

    try:
        logger.debug(f"DBæ¥ç¶š: {DB_CONFIG['host']}")
        if last_fetch_time:
            logger.debug(f"å¢—åˆ†å–å¾—: {last_fetch_time} ä»¥é™")
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute(query, params)
        rows = cur.fetchall()
        cur.close()
        conn.close()
        logger.debug(f"DBå–å¾—å®Œäº†: {len(rows)}ä»¶")
        return rows
    except Exception as e:
        logger.error(f"DBæ¥ç¶šå¤±æ•—: {e}")
        return []


# ======================
# ç”»åƒå‡¦ç†
# ======================
def backup_and_process(
    file_path: str,
    is_first_image: bool = False,
) -> dict:
    """
    ç”»åƒã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦å‡¦ç†

    ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯:
    1. .backupãƒ•ã‚©ãƒ«ãƒ€ãŒãªã‘ã‚Œã°ä½œæˆã—ã€å…ƒç”»åƒã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    2. .backupã«æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¸ˆã¿ï¼‰
    3. å‡¦ç†å®Ÿè¡Œ

    Args:
        file_path: S3ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: /upfile/1007/4856/20220824190333_1.jpg)
        is_first_image: æœ€åˆã®ç”»åƒã‹ã©ã†ã‹ (True=ãƒãƒŠãƒ¼è¿½åŠ )

    Returns:
        dict: å‡¦ç†çµæœ
    """
    # ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    full_path = os.path.join(S3_MOUNT, file_path.lstrip("/"))

    logger.debug(f"å‡¦ç†é–‹å§‹: {full_path}")

    if not os.path.exists(full_path):
        logger.warn(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªæ¤œå‡º: {full_path}")
        return {"status": "skip", "reason": "file_not_found", "path": full_path}

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‘ã‚¹è¨­å®š
    file_name = os.path.basename(full_path)
    relative_path = file_path.lstrip("/")  # upfile/1041/8430/xxx.jpg

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
    # å„ªå…ˆé †ä½: BACKUP_S3_BUCKET > BACKUP_DIR
    if BACKUP_S3_BUCKET:
        # === boto3 S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ï¼‰===
        # S3 key: webroot/upfile/1041/8430/.backup/xxx.jpg
        dir_part = os.path.dirname(relative_path)  # upfile/1041/8430
        s3_key = f"webroot/{dir_part}/.backup/{file_name}"

        try:
            backup_exists = s3_backup_exists(s3_key)

            if backup_exists:
                # S3ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã«å¾©å…ƒ
                logger.debug(
                    f"S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ: s3://{BACKUP_S3_BUCKET}/{s3_key}"
                )
                s3_download_backup(s3_key, full_path)
            else:
                # åˆå›: S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                logger.debug(f"S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: s3://{BACKUP_S3_BUCKET}/{s3_key}")
                s3_upload_backup(full_path, s3_key)
        except Exception as e:
            logger.error(f"S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            return {
                "status": "error",
                "reason": f"s3_backup_failed: {e}",
                "path": full_path,
            }
    elif BACKUP_DIR:
        # === ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— ===
        backup_path = os.path.join(BACKUP_DIR, relative_path)
        backup_dir = os.path.dirname(backup_path)
        backup_exists = os.path.exists(backup_path)

        if backup_exists:
            # ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰å¾©å…ƒ
            try:
                logger.debug(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ: {backup_path}")
                shutil.copy(backup_path, full_path)
            except Exception as e:
                logger.error(f"å¾©å…ƒå¤±æ•—: {e}")
                return {
                    "status": "error",
                    "reason": f"restore_failed: {e}",
                    "path": full_path,
                }
        else:
            # åˆå›: ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            try:
                if not os.path.exists(backup_dir):
                    os.makedirs(backup_dir, exist_ok=True)

                if os.path.exists(backup_path):
                    logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ—¢å­˜ï¼ˆä¸Šæ›¸ãç¦æ­¢ï¼‰: {backup_path}")
                else:
                    logger.debug(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
                    shutil.copy(full_path, backup_path)
            except Exception as e:
                logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
                return {
                    "status": "error",
                    "reason": f"backup_failed: {e}",
                    "path": full_path,
                }
    else:
        # ã©ã¡ã‚‰ã‚‚æœªè¨­å®šã¯ã‚¨ãƒ©ãƒ¼
        logger.error("BACKUP_S3_BUCKET ã¾ãŸã¯ BACKUP_DIR ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return {
            "status": "error",
            "reason": "no_backup_config",
            "path": full_path,
        }

    # .detect/ ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«å…±é€šã§ä½¿ç”¨ï¼‰
    dir_path = os.path.dirname(full_path)
    detect_dir = os.path.join(dir_path, ".detect")
    detect_output_path = os.path.join(detect_dir, file_name)

    # å‡¦ç†å®Ÿè¡Œï¼ˆTwo-Stage: Seg + Poseï¼‰
    try:
        # === å…¨ãƒ•ã‚¡ã‚¤ãƒ«å…±é€š: .detect/ ã«ãƒã‚¹ã‚¯ï¼‹ãƒãƒŠãƒ¼ç‰ˆã‚’ä¿å­˜ ===
        logger.debug(f"Two-Stageæ¨è«–é–‹å§‹: .detect/ å‡ºåŠ› (masking=True, banner=True)")
        result = process_image(
            input_path=full_path,
            output_path=detect_output_path,
            seg_model=seg_model,
            pose_model=pose_model,
            mask_image=mask_image,
            is_masking=True,  # ãƒã‚¹ã‚¯ã‚ã‚Š
            add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
        )

        # === First fileã®ã¿: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã ===
        if is_first_image:
            logger.debug(f"First file: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã")
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒã—ã¦ã‹ã‚‰ãƒãƒŠãƒ¼ã®ã¿é©ç”¨
            if BACKUP_S3_BUCKET:
                dir_part = os.path.dirname(relative_path)
                s3_key = f"webroot/{dir_part}/.backup/{file_name}"
                s3_download_backup(s3_key, full_path)
            elif BACKUP_DIR:
                backup_path = os.path.join(BACKUP_DIR, relative_path)
                if os.path.exists(backup_path):
                    shutil.copy(backup_path, full_path)

            # ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            process_image(
                input_path=full_path,
                output_path=full_path,
                seg_model=seg_model,
                pose_model=pose_model,
                mask_image=mask_image,
                is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
            )

        result["status"] = "success"
        result["output_path"] = detect_output_path
        result["is_first"] = is_first_image
        if is_first_image:
            result["original_output"] = full_path  # First fileã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‘ã‚¹æƒ…å ±
        if BACKUP_S3_BUCKET:
            dir_part = os.path.dirname(relative_path)
            result["backup_path"] = (
                f"s3://{BACKUP_S3_BUCKET}/webroot/{dir_part}/.backup/{file_name}"
            )
        elif BACKUP_DIR:
            result["backup_path"] = os.path.join(BACKUP_DIR, relative_path)

        logger.debug(
            f"å‡¦ç†å®Œäº†: æ¤œå‡ºæ•°={result.get('detections', 0)}, "
            f".detect={detect_output_path}"
            + (f", original={full_path}" if is_first_image else "")
        )
        return result
    except Exception as e:
        logger.error(f"å‡¦ç†å¤±æ•—: {e}")
        return {"status": "error", "reason": str(e), "path": full_path}


# ======================
# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
# ======================
def cleanup_old_logs(log_dir: Path, retention_days: int = 60):
    """å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    if not log_dir.exists():
        return

    cutoff_date = datetime.now() - timedelta(days=retention_days)
    deleted_count = 0

    # process.log.YYYYMMDD å½¢å¼ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
    for file_path in log_dir.glob("process.log.*"):
        try:
            if file_path.stat().st_mtime < cutoff_date.timestamp():
                file_path.unlink()
                deleted_count += 1
                logger.debug(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
        except OSError:
            pass

    # å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ*.logï¼‰
    for file_path in log_dir.glob("*.log"):
        if file_path.name == "process.log":
            continue  # ç¾åœ¨ã®ãƒ­ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—
        try:
            if file_path.stat().st_mtime < cutoff_date.timestamp():
                file_path.unlink()
                deleted_count += 1
                logger.debug(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
        except OSError:
            pass

    if deleted_count > 0:
        logger.info(
            f"ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {deleted_count}ä»¶å‰Šé™¤ ({retention_days}æ—¥ä»¥å‰)"
        )


# ======================
# è¨­å®šæ¤œè¨¼
# ======================
def validate_config() -> bool:
    """è¨­å®šã‚’æ¤œè¨¼"""
    errors = []

    if not DB_CONFIG["host"]:
        errors.append("DB_HOST ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not DB_CONFIG["user"]:
        errors.append("DB_USER ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not DB_CONFIG["password"]:
        errors.append("DB_PASSWORD ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not S3_MOUNT:
        errors.append("S3_MOUNT ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    elif not os.path.exists(S3_MOUNT):
        errors.append(f"S3_MOUNT ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {S3_MOUNT}")

    if errors:
        for error in errors:
            logger.error(f"è¨­å®šã‚¨ãƒ©ãƒ¼: {error}")
        logger.error(".env ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return False

    return True


# ======================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ======================
def main():
    # å¼•æ•°ãƒ‘ãƒ¼ã‚¹
    parser = argparse.ArgumentParser(
        description="è»Šä¸¡ç”»åƒã®ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒã‚¹ã‚­ãƒ³ã‚°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python fetch_today_images.py                    # ä»Šæ—¥ã®ç”»åƒã€æœ€å¤§10ä»¶
  python fetch_today_images.py --days-ago 1      # æ˜¨æ—¥ã®ç”»åƒ
  python fetch_today_images.py --date 2026-02-03 # ç‰¹å®šã®æ—¥ä»˜ã‚’æŒ‡å®š
  python fetch_today_images.py --limit 50        # æœ€å¤§50ä»¶å‡¦ç†
  python fetch_today_images.py --days-ago 7 --limit 100
        """,
    )
    parser.add_argument(
        "--days-ago",
        type=int,
        default=0,
        help="ä½•æ—¥å‰ã®ç”»åƒã‚’å‡¦ç†ã™ã‚‹ã‹ (0=ä»Šæ—¥, 1=æ˜¨æ—¥, ...) [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0]",
    )
    parser.add_argument(
        "--date",
        type=str,
        default=None,
        help="å‡¦ç†å¯¾è±¡æ—¥ã‚’æŒ‡å®š (YYYY-MM-DDå½¢å¼ã€--days-agoã‚ˆã‚Šå„ªå…ˆ)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=10,
        help="1å›ã®å®Ÿè¡Œã§å‡¦ç†ã™ã‚‹æœ€å¤§ç”»åƒæ•° [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10]",
    )

    args = parser.parse_args()

    # å¯¾è±¡æ—¥ã‚’è¨ˆç®—
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        except ValueError:
            logger.error(f"æ—¥ä»˜å½¢å¼ãŒä¸æ­£: {args.date} (YYYY-MM-DDå½¢å¼ã§æŒ‡å®š)")
            return
    else:
        target_date = datetime.now().date() - timedelta(days=args.days_ago)

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆ
    if BACKUP_S3_BUCKET:
        backup_location = f"s3://{BACKUP_S3_BUCKET}/webroot/.../.backup/ (boto3)"
    elif BACKUP_DIR:
        backup_location = f"{BACKUP_DIR} (ãƒ­ãƒ¼ã‚«ãƒ«)"
    else:
        backup_location = "æœªè¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰"

    logger.info("=" * 60)
    logger.info(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹ (Two-Stage)")
    logger.info(f"  å¯¾è±¡æ—¥: {target_date}")
    logger.info(f"  æœ€å¤§å‡¦ç†æ•°: {args.limit}ä»¶")
    logger.info(f"  S3ãƒã‚¦ãƒ³ãƒˆ: {S3_MOUNT}")
    logger.info(f"  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_location}")
    logger.info(f"  Segãƒ¢ãƒ‡ãƒ«: {SEG_MODEL_PATH}")
    logger.info(f"  Poseãƒ¢ãƒ‡ãƒ«: {POSE_MODEL_PATH}")
    logger.info("=" * 60)

    # è¨­å®šæ¤œè¨¼
    if not validate_config():
        return

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã®ã¿ï¼‰
    try:
        load_models()
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return

    # ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ60æ—¥ä»¥å‰ã‚’å‰Šé™¤ï¼‰
    tracker.cleanup_old_files(LOG_RETENTION_DAYS)
    cleanup_old_logs(LOG_DIR, LOG_RETENTION_DAYS)

    # æ—¢å­˜ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆ
    existing_stats = tracker.get_stats(target_date)
    logger.info(
        f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çŠ¶æ³: å‡¦ç†æ¸ˆã¿ {existing_stats['total']}ä»¶ "
        f"(æˆåŠŸ: {existing_stats['success']}, ã‚¨ãƒ©ãƒ¼: {existing_stats['error']})"
    )

    # æœ€å¾Œã®DBå–å¾—æ™‚åˆ»ã‚’å–å¾—ï¼ˆå¢—åˆ†å–å¾—ã§DBè² è·è»½æ¸›ï¼‰
    last_fetch_time = tracker.get_last_processed_time(target_date)
    if last_fetch_time:
        logger.info(
            f"å¢—åˆ†å–å¾—: {last_fetch_time.strftime('%H:%M:%S')} ä»¥é™ã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«"
        )
    else:
        logger.info("åˆå›å®Ÿè¡Œ: å…¨ä»¶å–å¾—")

    # ç”»åƒã‚’å–å¾—
    images = get_images_by_date(
        target_date=target_date, last_fetch_time=last_fetch_time
    )

    if not images:
        logger.info(f"{target_date} ã®ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    logger.info(f"DBå–å¾—: {len(images)}ä»¶")

    # è»Šä¸¡ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    car_images = {}
    for row in images:
        (
            file_id,
            car_cd,
            inspresultdata_cd,
            branch_no,
            save_file_name,
            created,
            modified,
        ) = row

        car_key = inspresultdata_cd if inspresultdata_cd else str(car_cd)

        if car_key not in car_images:
            car_images[car_key] = []

        car_images[car_key].append(
            {
                "id": file_id,
                "branch_no": branch_no,
                "path": save_file_name,
                "created": created,
                "modified": modified,
            }
        )

    logger.info(f"è»Šä¸¡æ•°: {len(car_images)}å°")

    # å‡¦ç†ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    stats = {"success": 0, "skip_tracked": 0, "skip_other": 0, "error": 0}
    processed_count = 0

    # è»Šä¸¡ã”ã¨ã®çµæœï¼ˆChatworké€šçŸ¥ç”¨ï¼‰
    car_results = []  # [(car_id, success, error, detections), ...]

    # å„è»Šä¸¡ã‚’å‡¦ç†
    for car_key, car_files in car_images.items():
        # limitåˆ°é”ãƒã‚§ãƒƒã‚¯
        if processed_count >= args.limit:
            logger.info(f"å‡¦ç†ä¸Šé™åˆ°é”: {args.limit}ä»¶")
            break

        # branch_noã§ã‚½ãƒ¼ãƒˆ
        car_files.sort(key=lambda x: x["branch_no"] or 999)

        # ã“ã®è»Šä¸¡ãŒæ—¢ã«ä¸€éƒ¨å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®šï¼‰
        # è»Šä¸¡ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—: /upfile/1041/8430/
        first_file_path = car_files[0]["path"]
        car_dir = os.path.dirname(first_file_path) + "/"  # /upfile/1041/8430/

        if tracker.has_car_any_processed(target_date, car_dir):
            stats["skip_tracked"] += len(car_files)
            logger.debug(f"è»Šä¸¡ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†ä¸­ï¼‰: {car_key} (ãƒ•ã‚©ãƒ«ãƒ€: {car_dir})")
            continue

        logger.debug(f"è»Šä¸¡å‡¦ç†é–‹å§‹: {car_key} ({len(car_files)}æš)")

        # è»Šä¸¡ã”ã¨ã®çµ±è¨ˆ
        car_success = 0
        car_error = 0
        car_detections = 0
        car_first_image_path = None  # æœ€åˆã«å‡¦ç†æˆåŠŸã—ãŸç”»åƒã®ãƒ‘ã‚¹

        for idx, file_info in enumerate(car_files):
            file_id = file_info["id"]

            # limitåˆ°é”ãƒã‚§ãƒƒã‚¯
            if processed_count >= args.limit:
                break

            # branch_no == 1 ã®ã¿ first file ã¨ã—ã¦æ‰±ã†
            is_first = file_info["branch_no"] == 1

            # å‡¦ç†å®Ÿè¡Œ
            result = backup_and_process(
                file_path=file_info["path"],
                is_first_image=is_first,
            )

            status = result.get("status", "error")

            if status == "success":
                stats["success"] += 1
                processed_count += 1
                car_success += 1
                car_detections += result.get("detections", 0)
                # branch_no=1ã®ç”»åƒãƒ‘ã‚¹ã‚’è¨˜éŒ²ï¼ˆãƒãƒŠãƒ¼ä»˜ãã®ä»£è¡¨ç”»åƒï¼‰
                if file_info["branch_no"] == 1:
                    car_first_image_path = file_info["path"]

                # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«è¨˜éŒ²
                tracker.mark_processed(
                    target_date=target_date,
                    file_id=file_id,
                    path=file_info["path"],
                    status="success",
                    detections=result.get("detections", 0),
                    is_first=is_first,
                    branch_no=file_info["branch_no"],
                )

                logger.success(
                    f"[{processed_count}/{args.limit}] {file_info['path']} "
                    f"(æ¤œå‡º: {result.get('detections', 0)}, "
                    f"ãƒãƒŠãƒ¼: {'ã‚ã‚Š' if is_first else 'ãªã—'})"
                )

            elif status == "error":
                stats["error"] += 1
                processed_count += 1
                car_error += 1

                # ã‚¨ãƒ©ãƒ¼ã‚‚ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«è¨˜éŒ²
                tracker.mark_processed(
                    target_date=target_date,
                    file_id=file_id,
                    path=file_info["path"],
                    status="error",
                    branch_no=file_info["branch_no"],
                    error_reason=result.get("reason", "unknown"),
                )

                logger.error(f"{file_info['path']} - {result.get('reason', 'unknown')}")

            else:  # skip
                stats["skip_other"] += 1
                logger.debug(
                    f"ã‚¹ã‚­ãƒƒãƒ—: {file_info['path']} - {result.get('reason', '')}"
                )

        # è»Šä¸¡å‡¦ç†å®Œäº†å¾Œã€çµæœã‚’è¨˜éŒ²ï¼ˆå‡¦ç†ãŒã‚ã£ãŸå ´åˆã®ã¿ï¼‰
        if car_success > 0 or car_error > 0:
            car_results.append(
                (car_key, car_success, car_error, car_detections, car_first_image_path)
            )

    # æœ€çµ‚çµ±è¨ˆ
    logger.info("=" * 60)
    logger.info("å‡¦ç†å®Œäº†")
    logger.info(f"  æˆåŠŸ: {stats['success']}ä»¶")
    logger.info(f"  ã‚¨ãƒ©ãƒ¼: {stats['error']}ä»¶")
    logger.info(f"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†æ¸ˆã¿ï¼‰: {stats['skip_tracked']}ä»¶")
    logger.info(f"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãã®ä»–ï¼‰: {stats['skip_other']}ä»¶")

    # æœ€çµ‚ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆ
    final_stats = tracker.get_stats(target_date)
    logger.info(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç´¯è¨ˆ: {final_stats['total']}ä»¶å‡¦ç†æ¸ˆã¿")

    # last_fetch_timeã‚’æ›´æ–°ï¼ˆæ¬¡å›ã¯æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾—ï¼‰
    tracker.set_last_processed_time(target_date, datetime.now())
    logger.info(f"æ¬¡å›å¢—åˆ†å–å¾—: {datetime.now().strftime('%H:%M:%S')} ä»¥é™")
    logger.info("=" * 60)

    # Chatworké€šçŸ¥ï¼ˆå‡¦ç†ãŒã‚ã£ãŸå ´åˆã®ã¿ï¼‰
    if CHATWORK_API_KEY and CHATWORK_ROOM_ID and car_results:
        message = build_processing_summary(target_date, stats, car_results)
        if send_chatwork_notification(message):
            logger.info("Chatworké€šçŸ¥é€ä¿¡å®Œäº†")
        else:
            logger.warn("Chatworké€šçŸ¥é€ä¿¡å¤±æ•—")


if __name__ == "__main__":
    main()
