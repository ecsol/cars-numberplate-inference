#!/usr/bin/env python3
"""
è»Šä¸¡ç”»åƒã‚’å–å¾—ã—ã€ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ãƒãƒƒãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. DBã‹ã‚‰æŒ‡å®šæ—¥ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã‚’å–å¾—
2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã§å‡¦ç†æ¸ˆã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
3. å„è»Šä¸¡ã®æœ€åˆã®ç”»åƒ(branch_no=1): ãƒã‚¹ã‚¯ + ãƒãƒŠãƒ¼è¿½åŠ 
4. ãã®ä»–ã®ç”»åƒ: ãƒã‚¹ã‚¯ã®ã¿
5. å…ƒç”»åƒã‚’.backupãƒ•ã‚©ãƒ«ãƒ€ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
6. å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«è¨˜éŒ²

Usage:
    python fetch_today_images.py                    # ä»Šæ—¥ã®ç”»åƒã€æœ€å¤§10ä»¶
    python fetch_today_images.py --days-ago 1      # æ˜¨æ—¥ã®ç”»åƒ
    python fetch_today_images.py --limit 50        # æœ€å¤§50ä»¶å‡¦ç†
    python fetch_today_images.py --days-ago 7 --limit 100
    python fetch_today_images.py --path /1554913G  # ç‰¹å®šãƒ•ã‚©ãƒ«ãƒ€ã‚’ç›´æ¥å‡¦ç†

crontab: * * * * * /path/to/venv/bin/python /path/to/fetch_today_images.py
"""

import argparse
import json
import os
import sys
import shutil
import warnings
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional

# Suppress boto3 Python 3.9 deprecation warning
warnings.filterwarnings("ignore", message=".*Boto3 will no longer support Python 3.9.*")

import boto3
from botocore.exceptions import ClientError
import psycopg2
import requests

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
SCRIPT_DIR = Path(__file__).parent
PROJECT_DIR = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_DIR))


def load_env_file():
    """ç’°å¢ƒå¤‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    env_file = PROJECT_DIR / ".env"
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    if key not in os.environ:
                        os.environ[key] = value


load_env_file()

from scripts.process_image_v2 import process_image

# ======================
# è¨­å®š
# ======================
DB_CONFIG = {
    "host": os.getenv("DB_HOST", ""),
    "database": os.getenv("DB_NAME", "cartrading"),
    "user": os.getenv("DB_USER", ""),
    "password": os.getenv("DB_PASSWORD", ""),
}

S3_MOUNT = os.getenv("S3_MOUNT", "")

# Two-Stage ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
SEG_MODEL_PATH = os.getenv(
    "SEG_MODEL_PATH", str(PROJECT_DIR / "models" / "best_yolo26x_lambda_20260201.pt")
)
POSE_MODEL_PATH = os.getenv(
    "POSE_MODEL_PATH", str(PROJECT_DIR / "models" / "yolo26x_pose_best.pt")
)
PLATE_MASK_PATH = os.getenv(
    "PLATE_MASK_PATH", str(PROJECT_DIR / "assets" / "plate_mask.png")
)

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€å†… logs/ï¼‰
_log_dir_env = os.getenv("LOG_DIR", "")
if _log_dir_env:
    LOG_DIR = Path(_log_dir_env)
    if not LOG_DIR.is_absolute():
        LOG_DIR = PROJECT_DIR / _log_dir_env
else:
    LOG_DIR = PROJECT_DIR / "logs"
LOG_FILE = LOG_DIR / "process.log"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
# BACKUP_S3_BUCKET: boto3ã§S3ã«ç›´æ¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ - mountpoint-s3ã®åˆ¶é™ã‚’å›é¿ï¼‰
# BACKUP_DIR: ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
# ã©ã¡ã‚‰ã‚‚æœªè¨­å®š: ã‚¨ãƒ©ãƒ¼
BACKUP_S3_BUCKET = os.getenv("BACKUP_S3_BUCKET", "")  # ä¾‹: cs1es3
BACKUP_S3_PREFIX = os.getenv("BACKUP_S3_PREFIX", ".backup")  # S3 key prefix
BACKUP_DIR = os.getenv("BACKUP_DIR", "")

# boto3 S3 client (lazy init)
_s3_client = None

# Chatworké€šçŸ¥è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
CHATWORK_API_KEY = os.getenv("CHATWORK_API_KEY", "")
CHATWORK_ROOM_ID = os.getenv("CHATWORK_ROOM_ID", "")
# ç”»åƒã®ãƒ™ãƒ¼ã‚¹URLï¼ˆChatworké€šçŸ¥ç”¨ï¼‰
IMAGE_BASE_URL = os.getenv("IMAGE_BASE_URL", "https://www.autobacs-cars-system.com")
# æ‹…å½“è€…ãƒªã‚¹ãƒˆï¼ˆChatworkãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç”¨ï¼‰
# ãƒ•ã‚¡ãƒ ã‚¿ã‚¤ã‚ºã‚ªãƒ³ (8892649) ã¯é™¤å¤–
CHATWORK_MENTION_USERS = [
    ("11055639", "BaoNTV"),
    ("11055644", "Nguyen Duc Thang"),
    ("11055661", "MinhDV"),
]


def get_s3_client():
    """S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰"""
    global _s3_client
    if _s3_client is None:
        _s3_client = boto3.client("s3")
    return _s3_client


def send_chatwork_notification(message: str) -> bool:
    """
    Chatworkã«é€šçŸ¥ã‚’é€ä¿¡

    Args:
        message: é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        bool: é€ä¿¡æˆåŠŸã‹ã©ã†ã‹
    """
    if not CHATWORK_API_KEY or not CHATWORK_ROOM_ID:
        return False

    try:
        url = f"https://api.chatwork.com/v2/rooms/{CHATWORK_ROOM_ID}/messages"
        headers = {"X-ChatWorkToken": CHATWORK_API_KEY}
        data = {"body": message}

        response = requests.post(url, headers=headers, data=data, timeout=10)
        response.raise_for_status()
        return True
    except Exception as e:
        logger.error(f"Chatworké€šçŸ¥å¤±æ•—: {e}")
        return False


def build_processing_summary(
    target_date: datetime.date,
    stats: dict,
    car_results: list,
) -> str:
    """
    å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ

    Args:
        target_date: å¯¾è±¡æ—¥
        stats: çµ±è¨ˆæƒ…å ±
        car_results: è»Šä¸¡ã”ã¨ã®å‡¦ç†çµæœ
            [(car_id, success_count, error_count, detections, images_list), ...]
            images_list: [(branch_no, path), ...] sorted by branch_no

    Returns:
        str: Chatworkç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    lines = [
        "[info][title]ğŸš— ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆå‡¦ç†å®Œäº†[/title]",
        f"ğŸ“… å¯¾è±¡æ—¥: {target_date}",
        f"âœ… æˆåŠŸ: {stats['success']}ä»¶",
        f"âŒ ã‚¨ãƒ©ãƒ¼: {stats['error']}ä»¶",
        f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {stats['skip_tracked'] + stats['skip_other']}ä»¶",
        "[/info]",
        "",
    ]

    if car_results:
        lines.append("[info][title]ğŸ“Š è»Šä¸¡åˆ¥çµæœ[/title]")
        for idx, (car_id, success, error, detections, car_images) in enumerate(
            car_results[:10]
        ):
            status_icon = "âœ…" if error == 0 else "âš ï¸"

            # æ‹…å½“è€…ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            if CHATWORK_MENTION_USERS:
                user_idx = idx % len(CHATWORK_MENTION_USERS)
                user_id, user_name = CHATWORK_MENTION_USERS[user_idx]
                mention = f"[To:{user_id}]{user_name}ã•ã‚“"
                lines.append(
                    f"{status_icon} {car_id}: {success}æšå‡¦ç†, æ¤œå‡º{detections}ä»¶ æ‹…å½“:{mention}"
                )
            else:
                lines.append(
                    f"{status_icon} {car_id}: {success}æšå‡¦ç†, æ¤œå‡º{detections}ä»¶"
                )

            # å…¨ç”»åƒã®URLã‚’branch_noé †ã§è¡¨ç¤ºï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ« + .detect/ãƒã‚¹ã‚¯æ¸ˆã¿ï¼‰
            # branch_noã§ã¯ãªãé€£ç•ªã§è¡¨ç¤ºï¼ˆ1ã‹ã‚‰é–‹å§‹ï¼‰
            for seq_no, (branch_no, path) in enumerate(car_images, start=1):
                dir_path = os.path.dirname(path)
                file_name = os.path.basename(path)
                # ã‚ªãƒªã‚¸ãƒŠãƒ«URL
                original_url = f"{IMAGE_BASE_URL}{path}"
                # ãƒã‚¹ã‚¯æ¸ˆã¿URL (.detect/)
                detect_url = f"{IMAGE_BASE_URL}{dir_path}/.detect/{file_name}"
                lines.append(f"  {seq_no}. å…ƒ: {original_url}")
                lines.append(f"     æ¤œ: {detect_url}")
            lines.append("")

        if len(car_results) > 10:
            lines.append(f"... ä»– {len(car_results) - 10}å°")
        lines.append("[/info]")

    return "\n".join(lines)


def s3_backup_exists(s3_key: str) -> bool:
    """S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
    try:
        get_s3_client().head_object(Bucket=BACKUP_S3_BUCKET, Key=s3_key)
        return True
    except ClientError as e:
        if e.response["Error"]["Code"] == "404":
            return False
        raise


def s3_upload_backup(local_path: str, s3_key: str):
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    get_s3_client().upload_file(local_path, BACKUP_S3_BUCKET, s3_key)


def s3_download_backup(s3_key: str, local_path: str):
    """S3ã‹ã‚‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆmountpoint-s3å¯¾å¿œï¼‰"""
    # mountpoint-s3ã¯shutil.copyãŒå‹•ã‹ãªã„ãŸã‚ã€
    # ãƒã‚¤ãƒˆå˜ä½ã§ç›´æ¥æ›¸ãè¾¼ã‚€
    response = get_s3_client().get_object(Bucket=BACKUP_S3_BUCKET, Key=s3_key)
    data = response["Body"].read()

    with open(local_path, "wb") as f:
        f.write(data)


# ======================
# ãƒ­ã‚®ãƒ³ã‚°
# ======================
class Logger:
    """è©³ç´°ãƒ­ã‚°å‡ºåŠ›ã‚¯ãƒ©ã‚¹"""

    def __init__(self, log_file: Path):
        self.log_file = log_file
        try:
            self.log_file.parent.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            self.log_file = Path("./process.log")
            print(f"[WARN] ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆä¸å¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {self.log_file}")

    def _write(self, level: str, message: str):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        log_line = f"[{timestamp}] [{level}] {message}"
        print(log_line)

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(log_line + "\n")
        except Exception:
            pass

    def info(self, message: str):
        self._write("INFO", message)

    def error(self, message: str):
        self._write("ERROR", message)

    def warn(self, message: str):
        self._write("WARN", message)

    def debug(self, message: str):
        self._write("DEBUG", message)

    def success(self, message: str):
        self._write("OK", message)


logger = Logger(LOG_FILE)

# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
LOG_RETENTION_DAYS = int(os.getenv("LOG_RETENTION_DAYS", "60"))


# ======================
# ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
# ======================
class ProcessingTracker:
    """
    å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°

    æ—¥ä»˜ã”ã¨ã«JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†:
    {PROJECT_DIR}/logs/tracking/processed_20260130.json

    å½¢å¼:
    {
        "date": "2026-01-30",
        "processed": {
            "12345": {
                "file_id": 12345,
                "path": "/upfile/1007/4856/xxx.jpg",
                "processed_at": "2026-01-30 12:34:56",
                "status": "success",
                "detections": 1,
                "is_first": true
            },
            ...
        }
    }
    """

    def __init__(self, log_dir: Path):
        self.tracking_dir = log_dir / "tracking"
        try:
            self.tracking_dir.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            self.tracking_dir = Path("./tracking")
            self.tracking_dir.mkdir(parents=True, exist_ok=True)

    def _get_tracking_file(self, target_date: datetime.date) -> Path:
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.tracking_dir / f"processed_{target_date.strftime('%Y%m%d')}.json"

    def load(self, target_date: datetime.date) -> dict:
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        tracking_file = self._get_tracking_file(target_date)

        if not tracking_file.exists():
            return {
                "date": target_date.isoformat(),
                "created_at": datetime.now().isoformat(),
                "processed": {},
            }

        try:
            with open(tracking_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return {
                "date": target_date.isoformat(),
                "created_at": datetime.now().isoformat(),
                "processed": {},
            }

    def save(self, target_date: datetime.date, data: dict):
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        tracking_file = self._get_tracking_file(target_date)
        data["updated_at"] = datetime.now().isoformat()

        try:
            with open(tracking_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {e}")

    def is_processed(self, target_date: datetime.date, file_id: int) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹"""
        data = self.load(target_date)
        return str(file_id) in data["processed"]

    def has_car_any_processed(
        self, target_date: datetime.date, car_path_prefix: str
    ) -> bool:
        """è»Šä¸¡ã®ã„ãšã‚Œã‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ï¼ˆãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒã‚§ãƒƒã‚¯ï¼‰

        Args:
            car_path_prefix: ä¾‹ "/upfile/1041/8430/"
        """
        data = self.load(target_date)
        for record in data.get("processed", {}).values():
            path = record.get("path", "")
            if path.startswith(car_path_prefix):
                return True
        return False

    def mark_processed(
        self,
        target_date: datetime.date,
        file_id: int,
        path: str,
        status: str,
        detections: int = 0,
        is_first: bool = False,
        branch_no: Optional[int] = None,
        car_id: Optional[str] = None,
        error_reason: Optional[str] = None,
    ):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        data = self.load(target_date)

        record = {
            "file_id": file_id,
            "car_id": car_id,
            "path": path,
            "branch_no": branch_no,
            "processed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "status": status,
            "detections": detections,
            "is_first": is_first,
        }

        if error_reason:
            record["error"] = error_reason

        data["processed"][str(file_id)] = record
        self.save(target_date, data)

    def get_stats(self, target_date: datetime.date) -> dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        data = self.load(target_date)
        processed = data.get("processed", {})

        stats = {
            "total": len(processed),
            "success": 0,
            "error": 0,
            "skip": 0,
        }

        for record in processed.values():
            status = record.get("status", "unknown")
            if status in stats:
                stats[status] += 1

        return stats

    def get_last_processed_time(self, target_date: datetime.date) -> Optional[datetime]:
        """æœ€å¾Œã«å‡¦ç†ã—ãŸæ™‚åˆ»ã‚’å–å¾—"""
        data = self.load(target_date)
        last_time_str = data.get("last_processed_time")
        if last_time_str:
            try:
                return datetime.fromisoformat(last_time_str)
            except ValueError:
                return None
        return None

    def set_last_processed_time(self, target_date: datetime.date, last_time: datetime):
        """æœ€å¾Œã«å‡¦ç†ã—ãŸæ™‚åˆ»ã‚’ä¿å­˜"""
        data = self.load(target_date)
        data["last_processed_time"] = last_time.isoformat()
        self.save(target_date, data)

    def cleanup_old_files(self, retention_days: int = 60):
        """å¤ã„ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        if not self.tracking_dir.exists():
            return

        cutoff_date = datetime.now().date() - timedelta(days=retention_days)
        deleted_count = 0

        for file_path in self.tracking_dir.glob("processed_*.json"):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º: processed_20260130.json
                date_str = file_path.stem.replace("processed_", "")
                file_date = datetime.strptime(date_str, "%Y%m%d").date()

                if file_date < cutoff_date:
                    file_path.unlink()
                    deleted_count += 1
                    logger.debug(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
            except (ValueError, OSError) as e:
                logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {file_path.name} - {e}")

        if deleted_count > 0:
            logger.info(
                f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {deleted_count}ä»¶å‰Šé™¤ ({retention_days}æ—¥ä»¥å‰)"
            )


tracker = ProcessingTracker(LOG_DIR)


# ======================
# ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒã‚¹ã‚¯èª­ã¿è¾¼ã¿ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ« - èµ·å‹•æ™‚ã«1å›ã®ã¿ï¼‰
# ======================
seg_model = None
pose_model = None
mask_image = None


def load_models():
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒã‚¹ã‚¯ç”»åƒã‚’èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã®ã¿ï¼‰"""
    global seg_model, pose_model, mask_image

    if seg_model is None:
        from ultralytics import YOLO

        logger.info(f"Segãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {SEG_MODEL_PATH}")
        seg_model = YOLO(SEG_MODEL_PATH)

    if pose_model is None:
        from ultralytics import YOLO

        logger.info(f"Poseãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {POSE_MODEL_PATH}")
        pose_model = YOLO(POSE_MODEL_PATH)

    if mask_image is None:
        import cv2

        logger.info(f"ãƒã‚¹ã‚¯ç”»åƒèª­ã¿è¾¼ã¿: {PLATE_MASK_PATH}")
        mask_image = cv2.imread(PLATE_MASK_PATH, cv2.IMREAD_UNCHANGED)
        if mask_image is None:
            raise ValueError(f"ãƒã‚¹ã‚¯ç”»åƒã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {PLATE_MASK_PATH}")

    return seg_model, pose_model, mask_image


# ======================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
# ======================
def get_images_from_path(folder_path: str) -> list:
    """
    æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆDBã‚’ãƒã‚¤ãƒ‘ã‚¹ï¼‰

    Args:
        folder_path: ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ (ä¾‹: /1554913G ã¾ãŸã¯ 1554913G)

    Returns:
        list: [(idx, None, car_id, branch_no, path, None, None), ...]
              get_images_by_dateã¨åŒã˜å½¢å¼ã§è¿”ã™
    """
    # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–
    folder_path = folder_path.strip("/")
    full_folder_path = os.path.join(S3_MOUNT, "upfile", folder_path)

    if not os.path.exists(full_folder_path):
        logger.error(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {full_folder_path}")
        return []

    # car_id ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ«ãƒ€åï¼‰
    car_id = os.path.basename(folder_path)

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    images = []
    image_extensions = {".jpg", ".jpeg", ".png", ".gif", ".webp"}

    for idx, file_name in enumerate(sorted(os.listdir(full_folder_path)), start=1):
        file_path = os.path.join(full_folder_path, file_name)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã€ç”»åƒæ‹¡å¼µå­ã®ã¿
        if not os.path.isfile(file_path):
            continue

        _, ext = os.path.splitext(file_name)
        if ext.lower() not in image_extensions:
            continue

        # .backup, .detect ãƒ•ã‚©ãƒ«ãƒ€å†…ã¯ã‚¹ã‚­ãƒƒãƒ—
        if ".backup" in file_path or ".detect" in file_path:
            continue

        # branch_noã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬ï¼ˆæœ«å°¾ã®æ•°å­—ï¼‰
        # ä¾‹: 1554913G133.jpg â†’ branch_no = 1 (æœ€åˆã®ç”»åƒ)
        # ã‚½ãƒ¼ãƒˆé †ã§æ±ºå®š: idx=1 ãŒ branch_no=1
        branch_no = idx

        # DBã¨åŒã˜å½¢å¼: /upfile/car_id/filename.jpg
        relative_path = f"/upfile/{folder_path}/{file_name}"

        images.append(
            (
                idx,  # id (ãƒ€ãƒŸãƒ¼)
                None,  # car_cd
                car_id,  # inspresultdata_cd
                branch_no,  # branch_no
                relative_path,  # save_file_name
                None,  # created
                None,  # modified
            )
        )

    logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€ã‚¹ã‚­ãƒ£ãƒ³: {full_folder_path} - {len(images)}æš")
    return images


def get_images_by_date(
    target_date: datetime.date,
    last_fetch_time: Optional[datetime] = None,
    only_first: bool = False,
) -> list:
    """
    æŒ‡å®šæ—¥ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã‚’å–å¾—

    Args:
        target_date: å¯¾è±¡æ—¥
        last_fetch_time: ã“ã®æ™‚åˆ»ä»¥é™ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã®ã¿å–å¾—ï¼ˆå¢—åˆ†å–å¾—ã§DBè² è·è»½æ¸›ï¼‰
        only_first: Trueã®å ´åˆã€branch_no=1ã®ã¿å–å¾—ï¼ˆ--force-overlayç”¨ï¼‰

    Returns:
        list: [(id, car_cd, inspresultdata_cd, branch_no, save_file_name, created, modified), ...]
    """

    # branch_no=1ã®ã¿å–å¾—ã™ã‚‹æ¡ä»¶
    branch_condition = "AND branch_no = 1" if only_first else ""

    if last_fetch_time:
        # å¢—åˆ†å–å¾—: last_fetch_timeä»¥é™ã®æ–°è¦/æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
        query = f"""
            SELECT 
                id,
                car_cd,
                inspresultdata_cd,
                branch_no,
                save_file_name,
                created,
                modified
            FROM upload_files
            WHERE (DATE(created) = %s OR DATE(modified) = %s)
              AND (created > %s OR modified > %s)
              AND delete_flg = 0
              AND save_file_name IS NOT NULL
              AND save_file_name != ''
              {branch_condition}
            ORDER BY 
                COALESCE(inspresultdata_cd, car_cd::text),
                branch_no ASC
        """
        params = (target_date, target_date, last_fetch_time, last_fetch_time)
    else:
        # åˆå›: å…¨ä»¶å–å¾—
        query = f"""
            SELECT 
                id,
                car_cd,
                inspresultdata_cd,
                branch_no,
                save_file_name,
                created,
                modified
            FROM upload_files
            WHERE (DATE(created) = %s OR DATE(modified) = %s)
              AND delete_flg = 0
              AND save_file_name IS NOT NULL
              AND save_file_name != ''
              {branch_condition}
            ORDER BY 
                COALESCE(inspresultdata_cd, car_cd::text),
                branch_no ASC
        """
        params = (target_date, target_date)

    try:
        logger.debug(f"DBæ¥ç¶š: {DB_CONFIG['host']}")
        logger.debug(f"å¯¾è±¡æ—¥: {target_date}, only_first: {only_first}")
        if last_fetch_time:
            logger.debug(f"å¢—åˆ†å–å¾—: {last_fetch_time} ä»¥é™")
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute(query, params)
        rows = cur.fetchall()
        cur.close()
        conn.close()
        logger.debug(f"DBå–å¾—å®Œäº†: {len(rows)}ä»¶")
        return rows
    except Exception as e:
        logger.error(f"DBæ¥ç¶šå¤±æ•—: {e}")
        return []


# ======================
# ç”»åƒå‡¦ç†
# ======================
def backup_and_process(
    file_path: str,
    is_first_image: bool = False,
    force_overlay: bool = False,
    force: bool = False,
) -> dict:
    """
    ç”»åƒã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦å‡¦ç†

    ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯:
    1. .backupãƒ•ã‚©ãƒ«ãƒ€ãŒãªã‘ã‚Œã°ä½œæˆã—ã€å…ƒç”»åƒã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    2. .backupã«æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¸ˆã¿ï¼‰
    3. å‡¦ç†å®Ÿè¡Œ

    Args:
        file_path: S3ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: /upfile/1007/4856/20220824190333_1.jpg)
        is_first_image: æœ€åˆã®ç”»åƒã‹ã©ã†ã‹ (True=ãƒãƒŠãƒ¼è¿½åŠ )
        force_overlay: å¼·åˆ¶çš„ã«ãƒãƒŠãƒ¼ã‚’è¿½åŠ ã™ã‚‹ã‹
        force: .detect/ãŒå­˜åœ¨ã—ã¦ã‚‚å¼·åˆ¶çš„ã«å†å‡¦ç†

    Returns:
        dict: å‡¦ç†çµæœ
    """
    # ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    full_path = os.path.join(S3_MOUNT, file_path.lstrip("/"))

    logger.debug(f"å‡¦ç†é–‹å§‹: {full_path}")

    if not os.path.exists(full_path):
        logger.warn(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªæ¤œå‡º: {full_path}")
        return {"status": "skip", "reason": "file_not_found", "path": full_path}

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‘ã‚¹è¨­å®š
    file_name = os.path.basename(full_path)
    relative_path = file_path.lstrip("/")  # upfile/1041/8430/xxx.jpg

    # --force ãƒ¢ãƒ¼ãƒ‰: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå…ƒç”»åƒã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
    if force:
        logger.debug(
            f"--force ãƒ¢ãƒ¼ãƒ‰: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚­ãƒƒãƒ—ã€ç¾åœ¨ã®ç”»åƒã‹ã‚‰.detect/ä½œæˆ"
        )
    else:
        # === é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: åˆå›ã®ã¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆå¾©å…ƒã¯æ‰‹å‹•restore_from_backup.pyã§ï¼‰ ===
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
        # å„ªå…ˆé †ä½: BACKUP_S3_BUCKET > BACKUP_DIR
        if BACKUP_S3_BUCKET:
            # === boto3 S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ï¼‰===
            # S3 key: webroot/upfile/1041/8430/.backup/xxx.jpg
            dir_part = os.path.dirname(relative_path)  # upfile/1041/8430
            s3_key = f"webroot/{dir_part}/.backup/{file_name}"

            try:
                backup_exists = s3_backup_exists(s3_key)

                if not backup_exists:
                    # åˆå›: S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                    logger.debug(
                        f"S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: s3://{BACKUP_S3_BUCKET}/{s3_key}"
                    )
                    s3_upload_backup(full_path, s3_key)
                # backup_exists ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆæ‰‹å‹•å¾©å…ƒç”¨ã«ä¿æŒï¼‰
            except Exception as e:
                logger.error(f"S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
                return {
                    "status": "error",
                    "reason": f"s3_backup_failed: {e}",
                    "path": full_path,
                }
        elif BACKUP_DIR:
            # === ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— ===
            backup_path = os.path.join(BACKUP_DIR, relative_path)
            backup_dir = os.path.dirname(backup_path)
            backup_exists = os.path.exists(backup_path)

            if not backup_exists:
                # åˆå›: ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                try:
                    if not os.path.exists(backup_dir):
                        os.makedirs(backup_dir, exist_ok=True)
                    logger.debug(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
                    shutil.copy(full_path, backup_path)
                except Exception as e:
                    logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
                    return {
                        "status": "error",
                        "reason": f"backup_failed: {e}",
                        "path": full_path,
                    }
            # backup_exists ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆæ‰‹å‹•å¾©å…ƒç”¨ã«ä¿æŒï¼‰
        else:
            # ã©ã¡ã‚‰ã‚‚æœªè¨­å®šã¯ã‚¨ãƒ©ãƒ¼
            logger.error("BACKUP_S3_BUCKET ã¾ãŸã¯ BACKUP_DIR ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            return {
                "status": "error",
                "reason": "no_backup_config",
                "path": full_path,
            }

    # .detect/ ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
    dir_part = os.path.dirname(relative_path)  # upfile/1041/8430

    # ãƒãƒŠãƒ¼ã®åˆ¤å®šï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰:
    # - First file (branch_no=1) ã®ã¿ãƒãƒŠãƒ¼è¿½åŠ 
    # - --force / --force-overlay ã¯åˆ¥å‡¦ç†
    add_banner_to_detect = is_first_image

    # å‡¦ç†å®Ÿè¡Œï¼ˆTwo-Stage: Seg + Poseï¼‰
    try:
        # === --force-overlay ãƒ¢ãƒ¼ãƒ‰: branch_no=1ã®ã¿å…ƒç”»åƒã«ç›´æ¥ãƒãƒŠãƒ¼ä¸Šæ›¸ã ===
        # ============================================================
        # --force-overlay ãƒ¢ãƒ¼ãƒ‰: å…ƒç”»åƒã«ç›´æ¥ãƒãƒŠãƒ¼ã‚’ä¸Šæ›¸ã
        # ============================================================
        # å‡¦ç†å†…å®¹:
        #   - branch_no=1 ã®ã¿: å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿ï¼ˆãƒã‚¹ã‚¯ãªã—ï¼‰ã§ä¸Šæ›¸ã
        #   - branch_no!=1: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†ã—ãªã„ï¼‰
        # å…¥åŠ›:
        #   - ç¾åœ¨ã®å…ƒç”»åƒï¼ˆfull_pathï¼‰ã‚’ãã®ã¾ã¾ä½¿ç”¨
        # å‡ºåŠ›:
        #   - å…ƒç”»åƒã‚’ç›´æ¥ä¸Šæ›¸ã
        #   - .detect/ ã¯ä½œæˆã—ãªã„
        # ============================================================
        if force_overlay:
            # branch_no=1 ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not is_first_image:
                logger.debug(f"--force-overlay: branch_no!=1 ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                return {
                    "status": "skip",
                    "reason": "force_overlay_not_first",
                    "path": full_path,
                }

            logger.debug(
                f"--force-overlay: å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿ä¸Šæ›¸ã (masking=False, banner=True)"
            )
            result = process_image(
                input_path=full_path,
                output_path=full_path,
                seg_model=seg_model,
                pose_model=pose_model,
                mask_image=mask_image,
                is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
            )
            result["status"] = "success"
            result["output_path"] = full_path
            result["is_first"] = is_first_image
            result["force_overlay"] = True
            logger.debug(f"--force-overlayå®Œäº†: {full_path}")
            return result

        # ============================================================
        # --force ãƒ¢ãƒ¼ãƒ‰: .detect/ ã‚’å¼·åˆ¶å†ä½œæˆï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãï¼‰
        # ============================================================
        # å‡¦ç†å†…å®¹:
        #   - branch_no=1 ã®ã¿: .detect/ ã«ãƒãƒŠãƒ¼ã®ã¿ï¼ˆãƒã‚¹ã‚¯ãªã—ï¼‰ã§ä¸Šæ›¸ã
        #   - branch_no!=1: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†ã—ãªã„ï¼‰
        # å…¥åŠ›:
        #   - .backup ã‹ã‚‰å…ƒç”»åƒã‚’å–å¾—ï¼ˆæ¤œå‡ºç²¾åº¦ã®ãŸã‚ï¼‰
        # å‡ºåŠ›:
        #   - .detect/ ã«ãƒãƒŠãƒ¼ã®ã¿ã®ç”»åƒã‚’ä¿å­˜
        #   - å…ƒç”»åƒã¯å¤‰æ›´ã—ãªã„
        # ============================================================
        if force:
            # branch_no=1 ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not is_first_image:
                logger.debug(f"--force: branch_no!=1 ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                return {
                    "status": "skip",
                    "reason": "force_not_first",
                    "path": full_path,
                }

            logger.debug(
                f"--force: .detect/ã«ãƒãƒŠãƒ¼ã®ã¿ä¸Šæ›¸ã (masking=False, banner=True)"
            )

            if BACKUP_S3_BUCKET:
                import tempfile

                # .backupã‹ã‚‰å…ƒç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¤œå‡ºç²¾åº¦ã®ãŸã‚ï¼‰
                backup_s3_key = f"webroot/{dir_part}/.backup/{file_name}"
                with tempfile.NamedTemporaryFile(
                    suffix=os.path.splitext(file_name)[1], delete=False
                ) as tmp:
                    temp_input_path = tmp.name

                if s3_backup_exists(backup_s3_key):
                    s3_download_backup(backup_s3_key, temp_input_path)
                    logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›: s3://{BACKUP_S3_BUCKET}/{backup_s3_key}")
                else:
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒãªã„å ´åˆã¯ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨
                    logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨: {full_path}")
                    shutil.copy(full_path, temp_input_path)

                with tempfile.NamedTemporaryFile(
                    suffix=os.path.splitext(file_name)[1], delete=False
                ) as tmp:
                    temp_detect_path = tmp.name

                result = process_image(
                    input_path=temp_input_path,
                    output_path=temp_detect_path,
                    seg_model=seg_model,
                    pose_model=pose_model,
                    mask_image=mask_image,
                    is_masking=False,  # ãƒã‚¹ã‚¯ãªã—ï¼ˆãƒãƒŠãƒ¼ã®ã¿ï¼‰
                    add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
                )

                detect_s3_key = f"webroot/{dir_part}/.detect/{file_name}"
                s3_upload_backup(temp_detect_path, detect_s3_key)
                logger.debug(
                    f".detect/ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{BACKUP_S3_BUCKET}/{detect_s3_key}"
                )
                os.unlink(temp_input_path)
                os.unlink(temp_detect_path)
                detect_output_path = f"s3://{BACKUP_S3_BUCKET}/{detect_s3_key}"
            else:
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›
                backup_path = os.path.join(BACKUP_DIR, relative_path) if BACKUP_DIR else None
                if backup_path and os.path.exists(backup_path):
                    input_path = backup_path
                    logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›: {backup_path}")
                else:
                    input_path = full_path
                    logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨: {full_path}")

                detect_dir = os.path.join(os.path.dirname(full_path), ".detect")
                detect_output_path = os.path.join(detect_dir, file_name)
                os.makedirs(detect_dir, exist_ok=True)

                result = process_image(
                    input_path=input_path,
                    output_path=detect_output_path,
                    seg_model=seg_model,
                    pose_model=pose_model,
                    mask_image=mask_image,
                    is_masking=False,  # ãƒã‚¹ã‚¯ãªã—ï¼ˆãƒãƒŠãƒ¼ã®ã¿ï¼‰
                    add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
                )

            result["status"] = "success"
            result["output_path"] = detect_output_path
            result["is_first"] = is_first_image
            result["force"] = True
            logger.debug(f"--forceå®Œäº†: {detect_output_path}")
            return result

        # ============================================================
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æ–°è¦ç”»åƒã‚’å‡¦ç†
        # ============================================================
        # å‡¦ç†å†…å®¹:
        #   - branch_no=1: .detect/ ã«ãƒã‚¹ã‚¯+ãƒãƒŠãƒ¼ã€å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿
        #   - branch_no!=1: .detect/ ã«ãƒã‚¹ã‚¯ã®ã¿ï¼ˆãƒãƒŠãƒ¼ãªã—ï¼‰ã€å…ƒç”»åƒã¯å¤‰æ›´ã—ãªã„
        # å…¥åŠ›:
        #   - .backup ã‹ã‚‰å…ƒç”»åƒã‚’å–å¾—ï¼ˆæ¤œå‡ºç²¾åº¦ã®ãŸã‚ï¼‰
        # å‡ºåŠ›:
        #   - .detect/ ã«ãƒã‚¹ã‚¯å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ä¿å­˜
        #   - branch_no=1 ã®ã¿å…ƒç”»åƒã«ãƒãƒŠãƒ¼è¿½åŠ 
        # æ¡ä»¶:
        #   - .detect/ ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        # ============================================================
        # .detect/ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        detect_check_path = os.path.join(
            os.path.dirname(full_path), ".detect", file_name
        )
        if os.path.exists(detect_check_path):
            logger.debug(f".detect/æ—¢å­˜ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {detect_check_path}")
            return {
                "status": "skip",
                "reason": "detect_exists",
                "path": full_path,
                "detect_path": detect_check_path,
            }

        logger.debug(
            f"Two-Stageæ¨è«–é–‹å§‹: .detect/ å‡ºåŠ› (masking=True, banner={add_banner_to_detect})"
        )

        # S3ã®å ´åˆ: tempãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›å¾Œã€boto3ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        # ãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆ: ç›´æ¥.detect/ã«å‡ºåŠ›
        if BACKUP_S3_BUCKET:
            import tempfile

            # .backupã‹ã‚‰å…ƒç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¤œå‡ºç²¾åº¦ã®ãŸã‚ï¼‰
            backup_s3_key = f"webroot/{dir_part}/.backup/{file_name}"
            with tempfile.NamedTemporaryFile(
                suffix=os.path.splitext(file_name)[1], delete=False
            ) as tmp:
                temp_input_path = tmp.name

            if s3_backup_exists(backup_s3_key):
                s3_download_backup(backup_s3_key, temp_input_path)
                logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›: s3://{BACKUP_S3_BUCKET}/{backup_s3_key}")
            else:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒãªã„å ´åˆã¯ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨
                logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨: {full_path}")
                shutil.copy(full_path, temp_input_path)

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
            with tempfile.NamedTemporaryFile(
                suffix=os.path.splitext(file_name)[1], delete=False
            ) as tmp:
                temp_detect_path = tmp.name

            result = process_image(
                input_path=temp_input_path,
                output_path=temp_detect_path,
                seg_model=seg_model,
                pose_model=pose_model,
                mask_image=mask_image,
                is_masking=True,  # ãƒã‚¹ã‚¯ã‚ã‚Š
                add_banner=add_banner_to_detect,  # First fileã®ã¿ãƒãƒŠãƒ¼
            )

            # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            detect_s3_key = f"webroot/{dir_part}/.detect/{file_name}"
            s3_upload_backup(temp_detect_path, detect_s3_key)
            logger.debug(
                f".detect/ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{BACKUP_S3_BUCKET}/{detect_s3_key}"
            )

            # First fileã®ã¿: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã
            if is_first_image:
                logger.debug(f"First file: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã")
                process_image(
                    input_path=temp_input_path,
                    output_path=full_path,
                    seg_model=seg_model,
                    pose_model=pose_model,
                    mask_image=mask_image,
                    is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                    add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
                )

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            os.unlink(temp_input_path)
            os.unlink(temp_detect_path)

            detect_output_path = f"s3://{BACKUP_S3_BUCKET}/{detect_s3_key}"
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›
            backup_path = os.path.join(BACKUP_DIR, relative_path) if BACKUP_DIR else None
            if backup_path and os.path.exists(backup_path):
                input_path = backup_path
                logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›: {backup_path}")
            else:
                input_path = full_path
                logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨: {full_path}")

            # ãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆã¯ç›´æ¥å‡ºåŠ›
            detect_dir = os.path.join(os.path.dirname(full_path), ".detect")
            detect_output_path = os.path.join(detect_dir, file_name)
            os.makedirs(detect_dir, exist_ok=True)

            result = process_image(
                input_path=input_path,
                output_path=detect_output_path,
                seg_model=seg_model,
                pose_model=pose_model,
                mask_image=mask_image,
                is_masking=True,  # ãƒã‚¹ã‚¯ã‚ã‚Š
                add_banner=add_banner_to_detect,  # First fileã®ã¿ãƒãƒŠãƒ¼
            )

            # First fileã®ã¿: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã
            if is_first_image:
                logger.debug(f"First file: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã")
                process_image(
                    input_path=input_path,
                    output_path=full_path,
                    seg_model=seg_model,
                    pose_model=pose_model,
                    mask_image=mask_image,
                    is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                    add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
                )

        result["status"] = "success"
        result["output_path"] = detect_output_path
        result["is_first"] = is_first_image
        if is_first_image:
            result["original_output"] = full_path  # First fileã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‘ã‚¹æƒ…å ±
        if BACKUP_S3_BUCKET:
            dir_part = os.path.dirname(relative_path)
            result["backup_path"] = (
                f"s3://{BACKUP_S3_BUCKET}/webroot/{dir_part}/.backup/{file_name}"
            )
        elif BACKUP_DIR:
            result["backup_path"] = os.path.join(BACKUP_DIR, relative_path)

        logger.debug(
            f"å‡¦ç†å®Œäº†: æ¤œå‡ºæ•°={result.get('detections', 0)}, "
            f".detect={detect_output_path}"
            + (f", original={full_path}" if is_first_image else "")
        )
        return result
    except Exception as e:
        logger.error(f"å‡¦ç†å¤±æ•—: {e}")
        return {"status": "error", "reason": str(e), "path": full_path}


# ======================
# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
# ======================
def cleanup_old_logs(log_dir: Path, retention_days: int = 60):
    """å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    if not log_dir.exists():
        return

    cutoff_date = datetime.now() - timedelta(days=retention_days)
    deleted_count = 0

    # process.log.YYYYMMDD å½¢å¼ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
    for file_path in log_dir.glob("process.log.*"):
        try:
            if file_path.stat().st_mtime < cutoff_date.timestamp():
                file_path.unlink()
                deleted_count += 1
                logger.debug(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
        except OSError:
            pass

    # å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ*.logï¼‰
    for file_path in log_dir.glob("*.log"):
        if file_path.name == "process.log":
            continue  # ç¾åœ¨ã®ãƒ­ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—
        try:
            if file_path.stat().st_mtime < cutoff_date.timestamp():
                file_path.unlink()
                deleted_count += 1
                logger.debug(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
        except OSError:
            pass

    if deleted_count > 0:
        logger.info(
            f"ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {deleted_count}ä»¶å‰Šé™¤ ({retention_days}æ—¥ä»¥å‰)"
        )


# ======================
# è¨­å®šæ¤œè¨¼
# ======================
def validate_config() -> bool:
    """è¨­å®šã‚’æ¤œè¨¼"""
    errors = []

    if not DB_CONFIG["host"]:
        errors.append("DB_HOST ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not DB_CONFIG["user"]:
        errors.append("DB_USER ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not DB_CONFIG["password"]:
        errors.append("DB_PASSWORD ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not S3_MOUNT:
        errors.append("S3_MOUNT ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    elif not os.path.exists(S3_MOUNT):
        errors.append(f"S3_MOUNT ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {S3_MOUNT}")

    if errors:
        for error in errors:
            logger.error(f"è¨­å®šã‚¨ãƒ©ãƒ¼: {error}")
        logger.error(".env ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return False

    return True


# ======================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ======================
def main():
    # å¼•æ•°ãƒ‘ãƒ¼ã‚¹
    parser = argparse.ArgumentParser(
        description="è»Šä¸¡ç”»åƒã®ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒã‚¹ã‚­ãƒ³ã‚°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python fetch_today_images.py                    # ä»Šæ—¥ã®ç”»åƒã€æœ€å¤§10ä»¶
  python fetch_today_images.py --days-ago 1      # æ˜¨æ—¥ã®ç”»åƒ
  python fetch_today_images.py --date 2026-02-03 # ç‰¹å®šã®æ—¥ä»˜ã‚’æŒ‡å®š
  python fetch_today_images.py --limit 50        # æœ€å¤§50ä»¶å‡¦ç†
  python fetch_today_images.py --days-ago 7 --limit 100
  python fetch_today_images.py --path /1554913G  # ç‰¹å®šãƒ•ã‚©ãƒ«ãƒ€ã‚’ç›´æ¥å‡¦ç†ï¼ˆDBãƒã‚¤ãƒ‘ã‚¹ï¼‰
  python fetch_today_images.py --path 1554913G   # å…ˆé ­/ã¯çœç•¥å¯
        """,
    )
    parser.add_argument(
        "--days-ago",
        type=int,
        default=0,
        help="ä½•æ—¥å‰ã®ç”»åƒã‚’å‡¦ç†ã™ã‚‹ã‹ (0=ä»Šæ—¥, 1=æ˜¨æ—¥, ...) [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0]",
    )
    parser.add_argument(
        "--date",
        type=str,
        default=None,
        help="å‡¦ç†å¯¾è±¡æ—¥ã‚’æŒ‡å®š (YYYY-MM-DDå½¢å¼ã€--days-agoã‚ˆã‚Šå„ªå…ˆ)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=10,
        help="1å›ã®å®Ÿè¡Œã§å‡¦ç†ã™ã‚‹æœ€å¤§è»Šä¸¡æ•° [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10]",
    )
    parser.add_argument(
        "--path",
        type=str,
        default=None,
        help="ç‰¹å®šãƒ•ã‚©ãƒ«ãƒ€ã®ã¿å‡¦ç† (ä¾‹: /1554913G ã¾ãŸã¯ 1554913G) - DBã‚’ãƒã‚¤ãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--force-overlay",
        action="store_true",
        help="å…¨ç”»åƒã«ãƒãƒŠãƒ¼ã‚’å¼·åˆ¶é©ç”¨ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…ˆé ­ç”»åƒã®ã¿)",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help=".detect/ãŒå­˜åœ¨ã—ã¦ã‚‚å¼·åˆ¶çš„ã«å†å‡¦ç†",
    )

    args = parser.parse_args()

    # å¯¾è±¡æ—¥ã‚’è¨ˆç®—
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        except ValueError:
            logger.error(f"æ—¥ä»˜å½¢å¼ãŒä¸æ­£: {args.date} (YYYY-MM-DDå½¢å¼ã§æŒ‡å®š)")
            return
    else:
        target_date = datetime.now().date() - timedelta(days=args.days_ago)

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆ
    if BACKUP_S3_BUCKET:
        backup_location = f"s3://{BACKUP_S3_BUCKET}/webroot/.../.backup/ (boto3)"
    elif BACKUP_DIR:
        backup_location = f"{BACKUP_DIR} (ãƒ­ãƒ¼ã‚«ãƒ«)"
    else:
        backup_location = "æœªè¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰"

    logger.info("=" * 60)
    logger.info(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹ (Two-Stage)")
    if args.path:
        logger.info(f"  å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€: {args.path}")
    else:
        logger.info(f"  å¯¾è±¡æ—¥: {target_date}")
    logger.info(f"  æœ€å¤§å‡¦ç†æ•°: {args.limit}ä»¶")
    logger.info(f"  S3ãƒã‚¦ãƒ³ãƒˆ: {S3_MOUNT}")
    logger.info(f"  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_location}")
    logger.info(f"  Segãƒ¢ãƒ‡ãƒ«: {SEG_MODEL_PATH}")
    logger.info(f"  Poseãƒ¢ãƒ‡ãƒ«: {POSE_MODEL_PATH}")
    logger.info("=" * 60)

    # è¨­å®šæ¤œè¨¼
    if not validate_config():
        return

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã®ã¿ï¼‰
    try:
        load_models()
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return

    # ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ60æ—¥ä»¥å‰ã‚’å‰Šé™¤ï¼‰
    tracker.cleanup_old_files(LOG_RETENTION_DAYS)
    cleanup_old_logs(LOG_DIR, LOG_RETENTION_DAYS)

    # --path ãƒ¢ãƒ¼ãƒ‰ã‹é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã‹ã§å‡¦ç†ã‚’åˆ†å²
    if args.path:
        # ãƒ•ã‚©ãƒ«ãƒ€ç›´æ¥æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ï¼ˆDBãƒã‚¤ãƒ‘ã‚¹ï¼‰
        logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€ç›´æ¥ãƒ¢ãƒ¼ãƒ‰: {args.path}")
        images = get_images_from_path(args.path)

        if not images:
            logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€ã«ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“: {args.path}")
            return
    else:
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆDBå–å¾—ï¼‰
        # æ—¢å­˜ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆ
        existing_stats = tracker.get_stats(target_date)
        logger.info(
            f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çŠ¶æ³: å‡¦ç†æ¸ˆã¿ {existing_stats['total']}ä»¶ "
            f"(æˆåŠŸ: {existing_stats['success']}, ã‚¨ãƒ©ãƒ¼: {existing_stats['error']})"
        )

        # æœ€å¾Œã®DBå–å¾—æ™‚åˆ»ã‚’å–å¾—ï¼ˆå¢—åˆ†å–å¾—ã§DBè² è·è»½æ¸›ï¼‰
        # --force-overlay ã®å ´åˆã¯å¢—åˆ†å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å…¨ä»¶å–å¾—
        if args.force_overlay:
            last_fetch_time = None
            logger.info("--force-overlay: å…¨ä»¶å–å¾—ãƒ¢ãƒ¼ãƒ‰")
        else:
            last_fetch_time = tracker.get_last_processed_time(target_date)
            if last_fetch_time:
                logger.info(
                    f"å¢—åˆ†å–å¾—: {last_fetch_time.strftime('%H:%M:%S')} ä»¥é™ã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«"
                )
            else:
                logger.info("åˆå›å®Ÿè¡Œ: å…¨ä»¶å–å¾—")

        # ç”»åƒã‚’å–å¾—
        images = get_images_by_date(
            target_date=target_date,
            last_fetch_time=last_fetch_time,
            only_first=args.force_overlay,  # --force-overlayã®å ´åˆã¯branch_no=1ã®ã¿
        )

        if not images:
            logger.info(f"{target_date} ã®ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“")
            return

        logger.info(f"DBå–å¾—: {len(images)}ä»¶")

    # è»Šä¸¡ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    car_images = {}
    for row in images:
        (
            file_id,
            car_cd,
            inspresultdata_cd,
            branch_no,
            save_file_name,
            created,
            modified,
        ) = row

        car_key = inspresultdata_cd if inspresultdata_cd else str(car_cd)

        if car_key not in car_images:
            car_images[car_key] = []

        car_images[car_key].append(
            {
                "id": file_id,
                "branch_no": branch_no,
                "path": save_file_name,
                "created": created,
                "modified": modified,
            }
        )

    total_cars = len(car_images)
    cars_to_process = min(total_cars, args.limit)
    logger.info(f"è»Šä¸¡æ•°: {total_cars}å° (å‡¦ç†äºˆå®š: {cars_to_process}å°)")

    # å‡¦ç†ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    stats = {"success": 0, "skip_tracked": 0, "skip_other": 0, "error": 0}
    processed_cars = 0  # å‡¦ç†ã—ãŸè»Šä¸¡æ•°ï¼ˆlimitã¯ã“ã‚Œã§åˆ¤å®šï¼‰

    # è»Šä¸¡ã”ã¨ã®çµæœï¼ˆChatworké€šçŸ¥ç”¨ï¼‰
    car_results = []  # [(car_id, success, error, detections), ...]

    # å„è»Šä¸¡ã‚’å‡¦ç†
    for car_key, car_files in car_images.items():
        # limitåˆ°é”ãƒã‚§ãƒƒã‚¯ï¼ˆè»Šä¸¡æ•°ã§åˆ¤å®šï¼‰
        if processed_cars >= args.limit:
            logger.info(f"å‡¦ç†ä¸Šé™åˆ°é”: {args.limit}å°")
            break

        # branch_noã§ã‚½ãƒ¼ãƒˆ
        car_files.sort(key=lambda x: x["branch_no"] or 999)

        # ã“ã®è»Šä¸¡ãŒæ—¢ã«ä¸€éƒ¨å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®šï¼‰
        # è»Šä¸¡ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—: /upfile/1041/8430/
        first_file_path = car_files[0]["path"]
        car_dir = os.path.dirname(first_file_path) + "/"  # /upfile/1041/8430/

        # --path ã¾ãŸã¯ --force-overlay ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ˜ç¤ºçš„ãªå†å‡¦ç†ï¼‰
        if (
            not args.path
            and not args.force_overlay
            and tracker.has_car_any_processed(target_date, car_dir)
        ):
            stats["skip_tracked"] += len(car_files)
            logger.debug(f"è»Šä¸¡ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†ä¸­ï¼‰: {car_key} (ãƒ•ã‚©ãƒ«ãƒ€: {car_dir})")
            continue

        logger.debug(f"è»Šä¸¡å‡¦ç†é–‹å§‹: {car_key} ({len(car_files)}æš)")

        # è»Šä¸¡ã”ã¨ã®çµ±è¨ˆ
        car_success = 0
        car_error = 0
        car_detections = 0
        car_images = []  # å‡¦ç†æˆåŠŸã—ãŸç”»åƒã®ãƒªã‚¹ãƒˆ [(branch_no, path), ...]

        for idx, file_info in enumerate(car_files):
            file_id = file_info["id"]

            # branch_no == 1 ã®ã¿ first file ã¨ã—ã¦æ‰±ã†
            is_first = file_info["branch_no"] == 1

            # å‡¦ç†å®Ÿè¡Œ
            result = backup_and_process(
                file_path=file_info["path"],
                is_first_image=is_first,
                force_overlay=args.force_overlay,
                force=args.force,
            )

            status = result.get("status", "error")

            if status == "success":
                stats["success"] += 1
                car_success += 1
                car_detections += result.get("detections", 0)
                # å‡¦ç†æˆåŠŸã—ãŸç”»åƒã‚’è¨˜éŒ²
                car_images.append((file_info["branch_no"] or 999, file_info["path"]))

                # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«è¨˜éŒ²ï¼ˆ--path/--force-overlayãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                if not args.path and not args.force_overlay:
                    tracker.mark_processed(
                        target_date=target_date,
                        file_id=file_id,
                        path=file_info["path"],
                        status="success",
                        detections=result.get("detections", 0),
                        is_first=is_first,
                        branch_no=file_info["branch_no"],
                        car_id=car_key,
                    )

                logger.success(
                    f"{file_info['path']} "
                    f"(æ¤œå‡º: {result.get('detections', 0)}, "
                    f"ãƒãƒŠãƒ¼: {'ã‚ã‚Š' if is_first else 'ãªã—'})"
                )

            elif status == "skip":
                # --force-overlay ã§ branch_no != 1 ã®å ´åˆãªã©ã‚¹ã‚­ãƒƒãƒ—
                stats["skipped"] = stats.get("skipped", 0) + 1
                logger.debug(
                    f"ã‚¹ã‚­ãƒƒãƒ—: {file_info['path']} - {result.get('reason', 'skip')}"
                )

            elif status == "error":
                stats["error"] += 1
                car_error += 1

                # ã‚¨ãƒ©ãƒ¼ã‚‚ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«è¨˜éŒ²ï¼ˆ--path/--force-overlayãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                if not args.path and not args.force_overlay:
                    tracker.mark_processed(
                        target_date=target_date,
                        file_id=file_id,
                        path=file_info["path"],
                        status="error",
                        branch_no=file_info["branch_no"],
                        car_id=car_key,
                        error_reason=result.get("reason", "unknown"),
                    )

                logger.error(f"{file_info['path']} - {result.get('reason', 'unknown')}")

            else:  # skip
                stats["skip_other"] += 1
                logger.debug(
                    f"ã‚¹ã‚­ãƒƒãƒ—: {file_info['path']} - {result.get('reason', '')}"
                )

        # è»Šä¸¡å‡¦ç†å®Œäº†å¾Œã€çµæœã‚’è¨˜éŒ²ï¼ˆå‡¦ç†ãŒã‚ã£ãŸå ´åˆã®ã¿ï¼‰
        if car_success > 0 or car_error > 0:
            processed_cars += 1  # è»Šä¸¡ã‚«ã‚¦ãƒ³ãƒˆ
            logger.info(
                f"[{processed_cars}/{cars_to_process}å°] {car_key}: "
                f"{car_success}æšæˆåŠŸ, {car_error}æšã‚¨ãƒ©ãƒ¼, æ¤œå‡º{car_detections}ä»¶"
            )
            # branch_noã§ã‚½ãƒ¼ãƒˆã—ã¦ã‹ã‚‰è¨˜éŒ²
            car_images.sort(key=lambda x: x[0])
            car_results.append(
                (car_key, car_success, car_error, car_detections, car_images)
            )

    # æœ€çµ‚çµ±è¨ˆ
    logger.info("=" * 60)
    logger.info("å‡¦ç†å®Œäº†")
    logger.info(f"  æˆåŠŸ: {stats['success']}ä»¶")
    logger.info(f"  ã‚¨ãƒ©ãƒ¼: {stats['error']}ä»¶")
    logger.info(f"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†æ¸ˆã¿ï¼‰: {stats['skip_tracked']}ä»¶")
    logger.info(f"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãã®ä»–ï¼‰: {stats['skip_other']}ä»¶")

    # --path ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if not args.path:
        # æœ€çµ‚ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆ
        final_stats = tracker.get_stats(target_date)
        logger.info(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç´¯è¨ˆ: {final_stats['total']}ä»¶å‡¦ç†æ¸ˆã¿")

        # last_fetch_timeã‚’æ›´æ–°ï¼ˆæ¬¡å›ã¯æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾—ï¼‰
        tracker.set_last_processed_time(target_date, datetime.now())
        logger.info(f"æ¬¡å›å¢—åˆ†å–å¾—: {datetime.now().strftime('%H:%M:%S')} ä»¥é™")
    logger.info("=" * 60)

    # Chatworké€šçŸ¥ï¼ˆå‡¦ç†ãŒã‚ã£ãŸå ´åˆã®ã¿ï¼‰
    if CHATWORK_API_KEY and CHATWORK_ROOM_ID and car_results:
        message = build_processing_summary(target_date, stats, car_results)
        if send_chatwork_notification(message):
            logger.info("Chatworké€šçŸ¥é€ä¿¡å®Œäº†")
        else:
            logger.warn("Chatworké€šçŸ¥é€ä¿¡å¤±æ•—")


if __name__ == "__main__":
    main()
