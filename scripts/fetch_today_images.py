#!/usr/bin/env python3
"""
è»Šä¸¡ç”»åƒã‚’å–å¾—ã—ã€ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ãƒãƒƒãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ã€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘ãƒãƒŠãƒ¼/ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã®è¿½åŠ æ¡ä»¶                              â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  â— branch_no=1 (å…ˆé ­ç”»åƒ) â†’ ãƒãƒŠãƒ¼è¿½åŠ OK                                 â•‘
â•‘  â— branch_no!=1 (2æšç›®ä»¥é™) â†’ ãƒãƒŠãƒ¼è¿½åŠ ã€çµ¶å¯¾ç¦æ­¢ã€‘ãƒã‚¹ã‚¯ã®ã¿           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. DBã‹ã‚‰æŒ‡å®šæ—¥ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã‚’å–å¾—
2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã§å‡¦ç†æ¸ˆã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
3. å„è»Šä¸¡ã®æœ€åˆã®ç”»åƒ(branch_no=1): ãƒã‚¹ã‚¯ + ãƒãƒŠãƒ¼è¿½åŠ 
4. ãã®ä»–ã®ç”»åƒ(branch_no!=1): ãƒã‚¹ã‚¯ã®ã¿ã€ãƒãƒŠãƒ¼ç¦æ­¢ã€‘
5. å…ƒç”»åƒã‚’.backupãƒ•ã‚©ãƒ«ãƒ€ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆåˆå›ã®ã¿ã€å¾©å…ƒã¯æ‰‹å‹•ï¼‰
6. å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«è¨˜éŒ²

Usage:
    python fetch_today_images.py                    # ä»Šæ—¥ã®ç”»åƒã€æœ€å¤§10ä»¶
    python fetch_today_images.py --days-ago 1      # æ˜¨æ—¥ã®ç”»åƒ
    python fetch_today_images.py --limit 50        # æœ€å¤§50ä»¶å‡¦ç†
    python fetch_today_images.py --days-ago 7 --limit 100
    python fetch_today_images.py --path /1554913G  # ç‰¹å®šãƒ•ã‚©ãƒ«ãƒ€ã‚’ç›´æ¥å‡¦ç†
    python fetch_today_images.py --force           # .detect/ + originalå¼·åˆ¶å†ä½œæˆ
    /home/ec2-user/plate-detection-service/venv/bin/python fetch_today_images.py --force-overlay   # å…ƒç”»åƒã«å¼·åˆ¶ãƒãƒŠãƒ¼ï¼ˆbranch_no=1ã®ã¿ï¼‰

crontab: * * * * * /path/to/venv/bin/python /path/to/fetch_today_images.py
"""

import argparse
import json
import os
import sys
import shutil
import warnings
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional

# Suppress boto3 Python 3.9 deprecation warning
warnings.filterwarnings("ignore", message=".*Boto3 will no longer support Python 3.9.*")

import boto3
from botocore.exceptions import ClientError
import psycopg2
import requests

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
SCRIPT_DIR = Path(__file__).parent
PROJECT_DIR = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_DIR))


def load_env_file():
    """ç’°å¢ƒå¤‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    env_file = PROJECT_DIR / ".env"
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    if key not in os.environ:
                        os.environ[key] = value


load_env_file()

from scripts.process_image_v2 import process_image

# ======================
# è¨­å®š
# ======================
DB_CONFIG = {
    "host": os.getenv("DB_HOST", ""),
    "database": os.getenv("DB_NAME", "cartrading"),
    "user": os.getenv("DB_USER", ""),
    "password": os.getenv("DB_PASSWORD", ""),
}

S3_MOUNT = os.getenv("S3_MOUNT", "")

# Two-Stage ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
SEG_MODEL_PATH = os.getenv(
    "SEG_MODEL_PATH", str(PROJECT_DIR / "models" / "best_yolo26x_lambda_20260201.pt")
)
POSE_MODEL_PATH = os.getenv(
    "POSE_MODEL_PATH", str(PROJECT_DIR / "models" / "yolo26x_pose_best.pt")
)
PLATE_MASK_PATH = os.getenv(
    "PLATE_MASK_PATH", str(PROJECT_DIR / "assets" / "plate_mask.png")
)

# æ¤œå‡ºã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã®branch_no
# ã“ã‚Œã‚‰ã®ç”»åƒã¯ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆæ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãã®ã¾ã¾.detect/ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹
# - 30: ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒ‘ãƒãƒ« (Photo30)
# - 31: ã‚³ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (Photo31)
# - 32: è»Šæ¤œè¨¼ (Photo32)
SKIP_DETECTION_BRANCH_NOS = {30, 31, 32}

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€å†… logs/ï¼‰
_log_dir_env = os.getenv("LOG_DIR", "")
if _log_dir_env:
    LOG_DIR = Path(_log_dir_env)
    if not LOG_DIR.is_absolute():
        LOG_DIR = PROJECT_DIR / _log_dir_env
else:
    LOG_DIR = PROJECT_DIR / "logs"
LOG_FILE = LOG_DIR / "process.log"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
# BACKUP_S3_BUCKET: boto3ã§S3ã«ç›´æ¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ - mountpoint-s3ã®åˆ¶é™ã‚’å›é¿ï¼‰
# BACKUP_DIR: ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
# ã©ã¡ã‚‰ã‚‚æœªè¨­å®š: ã‚¨ãƒ©ãƒ¼
BACKUP_S3_BUCKET = os.getenv("BACKUP_S3_BUCKET", "")  # ä¾‹: cs1es3
BACKUP_S3_PREFIX = os.getenv("BACKUP_S3_PREFIX", ".backup")  # S3 key prefix
BACKUP_DIR = os.getenv("BACKUP_DIR", "")

# boto3 S3 client (lazy init)
_s3_client = None

# Chatworké€šçŸ¥è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
CHATWORK_API_KEY = os.getenv("CHATWORK_API_KEY", "")
CHATWORK_ROOM_ID = os.getenv("CHATWORK_ROOM_ID", "")
# ç”»åƒã®ãƒ™ãƒ¼ã‚¹URLï¼ˆChatworké€šçŸ¥ç”¨ï¼‰
IMAGE_BASE_URL = os.getenv("IMAGE_BASE_URL", "https://www.autobacs-cars-system.com")
# æ‹…å½“è€…ãƒªã‚¹ãƒˆï¼ˆChatworkãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç”¨ï¼‰
# ãƒ•ã‚¡ãƒ ã‚¿ã‚¤ã‚ºã‚ªãƒ³ (8892649) ã¯é™¤å¤–
CHATWORK_MENTION_USERS = [
    ("11055639", "BaoNTV"),
    ("11055644", "Nguyen Duc Thang"),
    ("11055661", "MinhDV"),
]


def get_s3_client():
    """S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰"""
    global _s3_client
    if _s3_client is None:
        _s3_client = boto3.client("s3")
    return _s3_client


def send_chatwork_notification(message: str) -> bool:
    """
    Chatworkã«é€šçŸ¥ã‚’é€ä¿¡

    Args:
        message: é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        bool: é€ä¿¡æˆåŠŸã‹ã©ã†ã‹
    """
    if not CHATWORK_API_KEY or not CHATWORK_ROOM_ID:
        return False

    try:
        url = f"https://api.chatwork.com/v2/rooms/{CHATWORK_ROOM_ID}/messages"
        headers = {"X-ChatWorkToken": CHATWORK_API_KEY}
        data = {"body": message}

        response = requests.post(url, headers=headers, data=data, timeout=10)
        response.raise_for_status()
        return True
    except Exception as e:
        logger.error(f"Chatworké€šçŸ¥å¤±æ•—: {e}")
        return False


def build_processing_summary(
    target_date: datetime.date,
    stats: dict,
    car_results: list,
) -> str:
    """
    å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ

    Args:
        target_date: å¯¾è±¡æ—¥
        stats: çµ±è¨ˆæƒ…å ±
        car_results: è»Šä¸¡ã”ã¨ã®å‡¦ç†çµæœ
            [(car_id, success_count, error_count, detections, images_list), ...]
            images_list: [(branch_no, path), ...] sorted by branch_no

    Returns:
        str: Chatworkç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    lines = [
        "[info][title]ğŸš— ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆå‡¦ç†å®Œäº†[/title]",
        f"ğŸ“… å¯¾è±¡æ—¥: {target_date}",
        f"âœ… æˆåŠŸ: {stats['success']}ä»¶",
        f"âŒ ã‚¨ãƒ©ãƒ¼: {stats['error']}ä»¶",
        f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {stats['skip_tracked'] + stats['skip_other']}ä»¶",
        "[/info]",
        "",
    ]

    if car_results:
        lines.append("[info][title]ğŸ“Š è»Šä¸¡åˆ¥çµæœ[/title]")
        for idx, (car_id, success, error, detections, car_images) in enumerate(
            car_results[:10]
        ):
            status_icon = "âœ…" if error == 0 else "âš ï¸"

            # æ‹…å½“è€…ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            if CHATWORK_MENTION_USERS:
                user_idx = idx % len(CHATWORK_MENTION_USERS)
                user_id, user_name = CHATWORK_MENTION_USERS[user_idx]
                mention = f"[To:{user_id}]{user_name}ã•ã‚“"
                lines.append(
                    f"{status_icon} {car_id}: {success}æšå‡¦ç†, æ¤œå‡º{detections}ä»¶ æ‹…å½“:{mention}"
                )
            else:
                lines.append(
                    f"{status_icon} {car_id}: {success}æšå‡¦ç†, æ¤œå‡º{detections}ä»¶"
                )

            # å…¨ç”»åƒã®URLã‚’branch_noé †ã§è¡¨ç¤ºï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ« + .detect/ãƒã‚¹ã‚¯æ¸ˆã¿ï¼‰
            # branch_noã§ã¯ãªãé€£ç•ªã§è¡¨ç¤ºï¼ˆ1ã‹ã‚‰é–‹å§‹ï¼‰
            for seq_no, (branch_no, path) in enumerate(car_images, start=1):
                dir_path = os.path.dirname(path)
                file_name = os.path.basename(path)
                # ã‚ªãƒªã‚¸ãƒŠãƒ«URL
                original_url = f"{IMAGE_BASE_URL}{path}"
                # ãƒã‚¹ã‚¯æ¸ˆã¿URL (.detect/)
                detect_url = f"{IMAGE_BASE_URL}{dir_path}/.detect/{file_name}"
                lines.append(f"  {seq_no}. å…ƒ: {original_url}")
                lines.append(f"     æ¤œ: {detect_url}")
            lines.append("")

        if len(car_results) > 10:
            lines.append(f"... ä»– {len(car_results) - 10}å°")
        lines.append("[/info]")

    return "\n".join(lines)


def s3_backup_exists(s3_key: str) -> bool:
    """S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
    try:
        get_s3_client().head_object(Bucket=BACKUP_S3_BUCKET, Key=s3_key)
        return True
    except ClientError as e:
        if e.response["Error"]["Code"] == "404":
            return False
        raise


def s3_upload_backup(local_path: str, s3_key: str):
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    get_s3_client().upload_file(local_path, BACKUP_S3_BUCKET, s3_key)


def s3_download_backup(s3_key: str, local_path: str):
    """S3ã‹ã‚‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆmountpoint-s3å¯¾å¿œï¼‰"""
    # mountpoint-s3ã¯shutil.copyãŒå‹•ã‹ãªã„ãŸã‚ã€
    # ãƒã‚¤ãƒˆå˜ä½ã§ç›´æ¥æ›¸ãè¾¼ã‚€
    response = get_s3_client().get_object(Bucket=BACKUP_S3_BUCKET, Key=s3_key)
    data = response["Body"].read()

    with open(local_path, "wb") as f:
        f.write(data)


# ======================
# ãƒ­ã‚®ãƒ³ã‚°
# ======================
class Logger:
    """è©³ç´°ãƒ­ã‚°å‡ºåŠ›ã‚¯ãƒ©ã‚¹"""

    def __init__(self, log_file: Path):
        self.log_file = log_file
        try:
            self.log_file.parent.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            self.log_file = Path("./process.log")
            print(f"[WARN] ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆä¸å¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {self.log_file}")

    def _write(self, level: str, message: str):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        log_line = f"[{timestamp}] [{level}] {message}"
        print(log_line)

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(log_line + "\n")
        except Exception:
            pass

    def info(self, message: str):
        self._write("INFO", message)

    def error(self, message: str):
        self._write("ERROR", message)

    def warn(self, message: str):
        self._write("WARN", message)

    def debug(self, message: str):
        self._write("DEBUG", message)

    def success(self, message: str):
        self._write("OK", message)


logger = Logger(LOG_FILE)

# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
LOG_RETENTION_DAYS = int(os.getenv("LOG_RETENTION_DAYS", "60"))


# ======================
# ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
# ======================
# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å®šç¾©:
#   pending    - DBã‹ã‚‰å–å¾—ã€å‡¦ç†å¾…ã¡
#   processing - å‡¦ç†ä¸­
#   verified   - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªæ¸ˆã¿
#   done       - å®Œäº†ï¼ˆæˆåŠŸï¼‰
#   error      - ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ
TRACKING_STATUS = {
    "pending": "pending",
    "processing": "processing",
    "verified": "verified",
    "done": "done",
    "error": "error",
}


class ProcessingTracker:
    """
    å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼ˆçŠ¶æ…‹ç®¡ç†ä»˜ãï¼‰

    æ—¥ä»˜ã”ã¨ã«JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†:
    {PROJECT_DIR}/logs/tracking/processed_20260130.json

    ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ãƒ­ãƒ¼:
        pending â†’ processing â†’ verified â†’ done
                      â†“
                    error

    å½¢å¼:
    {
        "date": "2026-01-30",
        "processed": {
            "12345": {
                "file_id": 12345,
                "path": "/upfile/1007/4856/xxx.jpg",
                "status": "done",
                "status_history": [
                    {"status": "pending", "at": "2026-01-30 12:34:50"},
                    {"status": "processing", "at": "2026-01-30 12:34:51"},
                    {"status": "verified", "at": "2026-01-30 12:34:55"},
                    {"status": "done", "at": "2026-01-30 12:34:56"}
                ],
                "detections": 1,
                "is_first": true,
                "output_paths": {
                    "detect": "/upfile/.../xxx.jpg",
                    "original": "/upfile/.../xxx.jpg"  # branch_no=1ã®ã¿
                }
            },
            ...
        }
    }
    """

    def __init__(self, log_dir: Path):
        self.tracking_dir = log_dir / "tracking"
        try:
            self.tracking_dir.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            self.tracking_dir = Path("./tracking")
            self.tracking_dir.mkdir(parents=True, exist_ok=True)

    def _get_tracking_file(self, target_date: datetime.date) -> Path:
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.tracking_dir / f"processed_{target_date.strftime('%Y%m%d')}.json"

    def load(self, target_date: datetime.date) -> dict:
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        tracking_file = self._get_tracking_file(target_date)

        if not tracking_file.exists():
            return {
                "date": target_date.isoformat(),
                "created_at": datetime.now().isoformat(),
                "processed": {},
            }

        try:
            with open(tracking_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return {
                "date": target_date.isoformat(),
                "created_at": datetime.now().isoformat(),
                "processed": {},
            }

    def save(self, target_date: datetime.date, data: dict):
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        tracking_file = self._get_tracking_file(target_date)
        data["updated_at"] = datetime.now().isoformat()

        try:
            with open(tracking_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {e}")

    def get_status(self, target_date: datetime.date, file_id: int) -> Optional[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        data = self.load(target_date)
        record = data["processed"].get(str(file_id))
        if record:
            return record.get("status")
        return None

    def is_done(self, target_date: datetime.date, file_id: int) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Œäº†æ¸ˆã¿ï¼ˆdoneï¼‰ã‹ã©ã†ã‹"""
        status = self.get_status(target_date, file_id)
        return status == TRACKING_STATUS["done"]

    def is_processed(self, target_date: datetime.date, file_id: int) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ï¼ˆdone ã¾ãŸã¯ verifiedï¼‰ã‹ã©ã†ã‹"""
        status = self.get_status(target_date, file_id)
        return status in [TRACKING_STATUS["done"], TRACKING_STATUS["verified"]]

    def needs_processing(self, target_date: datetime.date, file_id: int) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†å¿…è¦ã‹ã©ã†ã‹ï¼ˆpending, processing, ã¾ãŸã¯æœªç™»éŒ²ï¼‰"""
        status = self.get_status(target_date, file_id)
        if status is None:
            return True
        return status in [TRACKING_STATUS["pending"], TRACKING_STATUS["processing"]]

    def has_car_any_done(
        self, target_date: datetime.date, car_path_prefix: str
    ) -> bool:
        """è»Šä¸¡ã®ã„ãšã‚Œã‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒdoneçŠ¶æ…‹ã‹ã©ã†ã‹ï¼ˆãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒã‚§ãƒƒã‚¯ï¼‰

        Args:
            car_path_prefix: ä¾‹ "/upfile/1041/8430/"
        """
        data = self.load(target_date)
        for record in data.get("processed", {}).values():
            path = record.get("path", "")
            status = record.get("status", "")
            if path.startswith(car_path_prefix) and status == TRACKING_STATUS["done"]:
                return True
        return False

    def has_car_all_done(
        self, target_date: datetime.date, car_path_prefix: str
    ) -> bool:
        """è»Šä¸¡ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒdoneçŠ¶æ…‹ã‹ã©ã†ã‹"""
        data = self.load(target_date)
        car_files = []
        for record in data.get("processed", {}).values():
            path = record.get("path", "")
            if path.startswith(car_path_prefix):
                car_files.append(record)

        if not car_files:
            return False

        return all(r.get("status") == TRACKING_STATUS["done"] for r in car_files)

    def _add_status_history(self, record: dict, new_status: str):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å±¥æ­´ã‚’è¿½åŠ """
        if "status_history" not in record:
            record["status_history"] = []
        record["status_history"].append({
            "status": new_status,
            "at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        })

    def mark_pending(
        self,
        target_date: datetime.date,
        file_id: int,
        path: str,
        branch_no: Optional[int] = None,
        car_id: Optional[str] = None,
        is_first: bool = False,
    ):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’pendingçŠ¶æ…‹ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        data = self.load(target_date)
        file_key = str(file_id)

        # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå†å‡¦ç†ã—ãªã„ï¼‰
        if file_key in data["processed"]:
            return

        record = {
            "file_id": file_id,
            "car_id": car_id,
            "path": path,
            "branch_no": branch_no,
            "is_first": is_first,
            "status": TRACKING_STATUS["pending"],
            "status_history": [],
        }
        self._add_status_history(record, TRACKING_STATUS["pending"])

        data["processed"][file_key] = record
        self.save(target_date, data)

    def mark_processing(self, target_date: datetime.date, file_id: int):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’processingçŠ¶æ…‹ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        data = self.load(target_date)
        file_key = str(file_id)

        if file_key not in data["processed"]:
            logger.warn(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_id} ãŒãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«å­˜åœ¨ã—ã¾ã›ã‚“")
            return

        record = data["processed"][file_key]
        record["status"] = TRACKING_STATUS["processing"]
        self._add_status_history(record, TRACKING_STATUS["processing"])

        self.save(target_date, data)

    def mark_verified(
        self,
        target_date: datetime.date,
        file_id: int,
        detections: int = 0,
        output_paths: Optional[dict] = None,
    ):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’verifiedçŠ¶æ…‹ã¨ã—ã¦ãƒãƒ¼ã‚¯ï¼ˆå‡ºåŠ›ç¢ºèªæ¸ˆã¿ï¼‰"""
        data = self.load(target_date)
        file_key = str(file_id)

        if file_key not in data["processed"]:
            logger.warn(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_id} ãŒãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«å­˜åœ¨ã—ã¾ã›ã‚“")
            return

        record = data["processed"][file_key]
        record["status"] = TRACKING_STATUS["verified"]
        record["detections"] = detections
        if output_paths:
            record["output_paths"] = output_paths
        self._add_status_history(record, TRACKING_STATUS["verified"])

        self.save(target_date, data)

    def mark_done(self, target_date: datetime.date, file_id: int):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’doneçŠ¶æ…‹ã¨ã—ã¦ãƒãƒ¼ã‚¯ï¼ˆå®Œäº†ï¼‰"""
        data = self.load(target_date)
        file_key = str(file_id)

        if file_key not in data["processed"]:
            logger.warn(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_id} ãŒãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«å­˜åœ¨ã—ã¾ã›ã‚“")
            return

        record = data["processed"][file_key]
        record["status"] = TRACKING_STATUS["done"]
        record["completed_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self._add_status_history(record, TRACKING_STATUS["done"])

        self.save(target_date, data)

    def mark_error(
        self,
        target_date: datetime.date,
        file_id: int,
        error_reason: str,
    ):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’errorçŠ¶æ…‹ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        data = self.load(target_date)
        file_key = str(file_id)

        if file_key not in data["processed"]:
            logger.warn(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_id} ãŒãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«å­˜åœ¨ã—ã¾ã›ã‚“")
            return

        record = data["processed"][file_key]
        record["status"] = TRACKING_STATUS["error"]
        record["error"] = error_reason
        self._add_status_history(record, TRACKING_STATUS["error"])

        self.save(target_date, data)

    def mark_car_done(self, target_date: datetime.date, car_path_prefix: str):
        """è»Šä¸¡ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’doneçŠ¶æ…‹ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        data = self.load(target_date)

        for file_key, record in data["processed"].items():
            path = record.get("path", "")
            status = record.get("status", "")
            if path.startswith(car_path_prefix) and status == TRACKING_STATUS["verified"]:
                record["status"] = TRACKING_STATUS["done"]
                record["completed_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                self._add_status_history(record, TRACKING_STATUS["done"])

        self.save(target_date, data)

    # Legacy method for backward compatibility
    def has_car_any_processed(
        self, target_date: datetime.date, car_path_prefix: str
    ) -> bool:
        """è»Šä¸¡ã®ã„ãšã‚Œã‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰"""
        return self.has_car_any_done(target_date, car_path_prefix)

    def mark_processed(
        self,
        target_date: datetime.date,
        file_id: int,
        path: str,
        status: str,
        detections: int = 0,
        is_first: bool = False,
        branch_no: Optional[int] = None,
        car_id: Optional[str] = None,
        error_reason: Optional[str] = None,
    ):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰"""
        data = self.load(target_date)

        record = {
            "file_id": file_id,
            "car_id": car_id,
            "path": path,
            "branch_no": branch_no,
            "processed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "status": status,
            "detections": detections,
            "is_first": is_first,
        }

        if error_reason:
            record["error"] = error_reason

        data["processed"][str(file_id)] = record
        self.save(target_date, data)

    def get_stats(self, target_date: datetime.date) -> dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        data = self.load(target_date)
        processed = data.get("processed", {})

        stats = {
            "total": len(processed),
            "pending": 0,
            "processing": 0,
            "verified": 0,
            "done": 0,
            "error": 0,
            # Legacy compatibility
            "success": 0,
            "skip": 0,
        }

        for record in processed.values():
            status = record.get("status", "unknown")
            if status in stats:
                stats[status] += 1
            # Legacy: count 'done' as 'success' too
            if status == "done":
                stats["success"] += 1

        return stats

    def get_last_processed_time(self, target_date: datetime.date) -> Optional[datetime]:
        """æœ€å¾Œã«å‡¦ç†ã—ãŸæ™‚åˆ»ã‚’å–å¾—"""
        data = self.load(target_date)
        last_time_str = data.get("last_processed_time")
        if last_time_str:
            try:
                return datetime.fromisoformat(last_time_str)
            except ValueError:
                return None
        return None

    def set_last_processed_time(self, target_date: datetime.date, last_time: datetime):
        """æœ€å¾Œã«å‡¦ç†ã—ãŸæ™‚åˆ»ã‚’ä¿å­˜"""
        data = self.load(target_date)
        data["last_processed_time"] = last_time.isoformat()
        self.save(target_date, data)

    def cleanup_old_files(self, retention_days: int = 60):
        """å¤ã„ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        if not self.tracking_dir.exists():
            return

        cutoff_date = datetime.now().date() - timedelta(days=retention_days)
        deleted_count = 0

        for file_path in self.tracking_dir.glob("processed_*.json"):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º: processed_20260130.json
                date_str = file_path.stem.replace("processed_", "")
                file_date = datetime.strptime(date_str, "%Y%m%d").date()

                if file_date < cutoff_date:
                    file_path.unlink()
                    deleted_count += 1
                    logger.debug(f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
            except (ValueError, OSError) as e:
                logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {file_path.name} - {e}")

        if deleted_count > 0:
            logger.info(
                f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {deleted_count}ä»¶å‰Šé™¤ ({retention_days}æ—¥ä»¥å‰)"
            )


tracker = ProcessingTracker(LOG_DIR)


# ======================
# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
# ======================
def verify_output_exists(
    file_path: str,
    is_first_image: bool = False,
) -> dict:
    """
    å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹æ¤œè¨¼

    Args:
        file_path: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: /upfile/1041/8430/xxx.jpg)
        is_first_image: æœ€åˆã®ç”»åƒã‹ã©ã†ã‹

    Returns:
        dict: {
            "verified": bool,
            "detect_exists": bool,
            "detect_path": str,
            "original_exists": bool,  # is_first_imageã®å ´åˆã®ã¿
            "original_path": str,
            "missing": list  # æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        }
    """
    full_path = os.path.join(S3_MOUNT, file_path.lstrip("/"))
    file_name = os.path.basename(full_path)
    dir_path = os.path.dirname(full_path)

    result = {
        "verified": False,
        "detect_exists": False,
        "detect_path": None,
        "original_exists": False,
        "original_path": full_path,
        "missing": [],
    }

    # .detect/ ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
    detect_path = os.path.join(dir_path, ".detect", file_name)
    result["detect_path"] = detect_path

    if BACKUP_S3_BUCKET:
        # S3ã®å ´åˆã¯boto3ã§ç¢ºèª
        relative_path = file_path.lstrip("/")
        dir_part = os.path.dirname(relative_path)
        detect_s3_key = f"webroot/{dir_part}/.detect/{file_name}"
        try:
            result["detect_exists"] = s3_backup_exists(detect_s3_key)
        except Exception:
            result["detect_exists"] = False
    else:
        result["detect_exists"] = os.path.exists(detect_path)

    if not result["detect_exists"]:
        result["missing"].append(detect_path)

    # First imageã®å ´åˆã¯originalã‚‚ãƒã‚§ãƒƒã‚¯
    if is_first_image:
        if BACKUP_S3_BUCKET:
            relative_path = file_path.lstrip("/")
            original_s3_key = f"webroot/{relative_path}"
            try:
                result["original_exists"] = s3_backup_exists(original_s3_key)
            except Exception:
                result["original_exists"] = False
        else:
            result["original_exists"] = os.path.exists(full_path)

        # originalã¯å¸¸ã«å­˜åœ¨ã™ã‚‹ã¯ãšãªã®ã§ã€missing ã«ã¯è¿½åŠ ã—ãªã„
        # ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯åˆ¥ã®å•é¡Œï¼‰

    # å…¨ã¦å­˜åœ¨ã™ã‚Œã°verified
    if is_first_image:
        result["verified"] = result["detect_exists"] and result["original_exists"]
    else:
        result["verified"] = result["detect_exists"]

    return result


# ======================
# ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒã‚¹ã‚¯èª­ã¿è¾¼ã¿ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ« - èµ·å‹•æ™‚ã«1å›ã®ã¿ï¼‰
# ======================
seg_model = None
pose_model = None
mask_image = None


def load_models():
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒã‚¹ã‚¯ç”»åƒã‚’èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã®ã¿ï¼‰"""
    global seg_model, pose_model, mask_image

    if seg_model is None:
        from ultralytics import YOLO

        logger.info(f"Segãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {SEG_MODEL_PATH}")
        seg_model = YOLO(SEG_MODEL_PATH)

    if pose_model is None:
        from ultralytics import YOLO

        logger.info(f"Poseãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {POSE_MODEL_PATH}")
        pose_model = YOLO(POSE_MODEL_PATH)

    if mask_image is None:
        import cv2

        logger.info(f"ãƒã‚¹ã‚¯ç”»åƒèª­ã¿è¾¼ã¿: {PLATE_MASK_PATH}")
        mask_image = cv2.imread(PLATE_MASK_PATH, cv2.IMREAD_UNCHANGED)
        if mask_image is None:
            raise ValueError(f"ãƒã‚¹ã‚¯ç”»åƒã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {PLATE_MASK_PATH}")

    return seg_model, pose_model, mask_image


# ======================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
# ======================
def get_images_from_path(folder_path: str) -> list:
    """
    æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆDBã‹ã‚‰branch_noã‚’å–å¾—ï¼‰

    ã€é‡è¦ã€‘branch_noã¯å¿…ãšDBã‹ã‚‰å–å¾—ã™ã‚‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬ã—ãªã„ï¼‰

    Args:
        folder_path: ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ (ä¾‹: /1554913G ã¾ãŸã¯ 1554913G)

    Returns:
        list: [(id, car_cd, inspresultdata_cd, branch_no, save_file_name, created, modified), ...]
              get_images_by_dateã¨åŒã˜å½¢å¼ã§è¿”ã™

    Note:
        DBã«å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹
    """
    # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–
    folder_path = folder_path.strip("/")
    full_folder_path = os.path.join(S3_MOUNT, "upfile", folder_path)

    if not os.path.exists(full_folder_path):
        logger.error(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {full_folder_path}")
        return []

    # car_id ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ«ãƒ€åï¼‰
    car_id = os.path.basename(folder_path)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆå­˜åœ¨ç¢ºèªç”¨ï¼‰
    image_extensions = {".jpg", ".jpeg", ".png", ".gif", ".webp"}
    file_names = []

    for file_name in sorted(os.listdir(full_folder_path)):
        file_path = os.path.join(full_folder_path, file_name)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã€ç”»åƒæ‹¡å¼µå­ã®ã¿
        if not os.path.isfile(file_path):
            continue

        _, ext = os.path.splitext(file_name)
        if ext.lower() not in image_extensions:
            continue

        # .backup, .detect ãƒ•ã‚©ãƒ«ãƒ€å†…ã¯ã‚¹ã‚­ãƒƒãƒ—
        if ".backup" in file_path or ".detect" in file_path:
            continue

        file_names.append(file_name)

    if not file_names:
        logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€ã‚¹ã‚­ãƒ£ãƒ³: {full_folder_path} - ç”»åƒãªã—")
        return []

    # DBã‹ã‚‰branch_noã‚’å–å¾—ï¼ˆçµ¶å¯¾ã«ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬ã—ãªã„ï¼ï¼‰
    # ãƒ•ã‚©ãƒ«ãƒ€IDã«ã¯2ç¨®é¡ã‚ã‚‹:
    #   1. car_cdå½¢å¼: 1041/9302 (æ•°å€¤/æ•°å€¤)
    #   2. inspresultdata_cdå½¢å¼: 1555316G (è‹±æ•°å­—)
    # ä¸¡æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ã™ã‚‹
    like_pattern = f"/upfile/{folder_path}/%"

    # inspresultdata_cdã®å ´åˆã¯ãƒ•ã‚©ãƒ«ãƒ€åãŒãã®ã¾ã¾ID
    # car_cdã®å ´åˆã¯ãƒ•ã‚©ãƒ«ãƒ€åã®æœ€å¾Œã®éƒ¨åˆ†ãŒID (ä¾‹: 1041/9302 â†’ 9302)
    folder_id = os.path.basename(folder_path)

    query = """
        SELECT 
            id,
            car_cd,
            inspresultdata_cd,
            branch_no,
            save_file_name,
            created,
            modified
        FROM upload_files
        WHERE (save_file_name LIKE %s OR inspresultdata_cd = %s)
          AND delete_flg = 0
        ORDER BY branch_no ASC
    """

    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute(query, (like_pattern, folder_id))
        rows = cur.fetchall()
        cur.close()
        conn.close()

        # DBã®çµæœã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆbranch_noã¯DBã®å€¤ã‚’ä½¿ç”¨ï¼‰
        logger.info(
            f"ãƒ•ã‚©ãƒ«ãƒ€ã‚¹ã‚­ãƒ£ãƒ³: {full_folder_path} - "
            f"ãƒ•ã‚¡ã‚¤ãƒ«{len(file_names)}æš, DBç™»éŒ²{len(rows)}ä»¶"
        )

        if len(rows) != len(file_names):
            logger.warn(
                f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¨DBç™»éŒ²æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“: "
                f"ãƒ•ã‚¡ã‚¤ãƒ«={len(file_names)}, DB={len(rows)}"
            )

        return rows

    except Exception as e:
        logger.error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return []


def get_images_by_date(
    target_date: datetime.date,
    last_fetch_time: Optional[datetime] = None,
    only_first: bool = False,
    order: str = "newest",
) -> list:
    """
    æŒ‡å®šæ—¥ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã‚’å–å¾—

    Args:
        target_date: å¯¾è±¡æ—¥
        last_fetch_time: ã“ã®æ™‚åˆ»ä»¥é™ã«ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸç”»åƒã®ã¿å–å¾—ï¼ˆå¢—åˆ†å–å¾—ã§DBè² è·è»½æ¸›ï¼‰
        only_first: Trueã®å ´åˆã€branch_no=1ã®ã¿å–å¾—ï¼ˆ--force-overlayç”¨ï¼‰
        order: ä¸¦ã³é † "newest"=æ–°ã—ã„é †, "oldest"=å¤ã„é †

    Returns:
        list: [(id, car_cd, inspresultdata_cd, branch_no, save_file_name, created, modified), ...]
    """

    # branch_no=1ã®ã¿å–å¾—ã™ã‚‹æ¡ä»¶
    branch_condition = "AND branch_no = 1" if only_first else ""

    # ä¸¦ã³é †: newest=æ–°ã—ã„é †(DESC), oldest=å¤ã„é †(ASC)
    date_order = "DESC" if order == "newest" else "ASC"

    if last_fetch_time:
        # å¢—åˆ†å–å¾—: last_fetch_timeä»¥é™ã®æ–°è¦/æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
        query = f"""
            SELECT 
                id,
                car_cd,
                inspresultdata_cd,
                branch_no,
                save_file_name,
                created,
                modified
            FROM upload_files
            WHERE (DATE(created) = %s OR DATE(modified) = %s)
              AND (created > %s OR modified > %s)
              AND delete_flg = 0
              AND save_file_name IS NOT NULL
              AND save_file_name != ''
              {branch_condition}
            ORDER BY 
                GREATEST(created, modified) {date_order},
                COALESCE(inspresultdata_cd, car_cd::text),
                branch_no ASC
        """
        params = (target_date, target_date, last_fetch_time, last_fetch_time)
    else:
        # åˆå›: å…¨ä»¶å–å¾—
        query = f"""
            SELECT 
                id,
                car_cd,
                inspresultdata_cd,
                branch_no,
                save_file_name,
                created,
                modified
            FROM upload_files
            WHERE (DATE(created) = %s OR DATE(modified) = %s)
              AND delete_flg = 0
              AND save_file_name IS NOT NULL
              AND save_file_name != ''
              {branch_condition}
            ORDER BY 
                GREATEST(created, modified) {date_order},
                COALESCE(inspresultdata_cd, car_cd::text),
                branch_no ASC
        """
        params = (target_date, target_date)

    try:
        logger.debug(f"DBæ¥ç¶š: {DB_CONFIG['host']}")
        logger.debug(f"å¯¾è±¡æ—¥: {target_date}, only_first: {only_first}")
        if last_fetch_time:
            logger.debug(f"å¢—åˆ†å–å¾—: {last_fetch_time} ä»¥é™")
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute(query, params)
        rows = cur.fetchall()
        cur.close()
        conn.close()
        logger.debug(f"DBå–å¾—å®Œäº†: {len(rows)}ä»¶")
        return rows
    except Exception as e:
        logger.error(f"DBæ¥ç¶šå¤±æ•—: {e}")
        return []


# ======================
# ç”»åƒå‡¦ç†
# ======================
def backup_and_process(
    file_path: str,
    is_first_image: bool = False,
    force_overlay: bool = False,
    force: bool = False,
    branch_no: Optional[int] = None,
) -> dict:
    """
    ç”»åƒã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦å‡¦ç†

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  ã€é‡è¦ã€‘ãƒãƒŠãƒ¼/ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã®ãƒ«ãƒ¼ãƒ«                              â•‘
    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
    â•‘  â— branch_no=1 (å…ˆé ­ç”»åƒ):                                       â•‘
    â•‘    â†’ ãƒãƒŠãƒ¼è¿½åŠ OKï¼ˆå…ƒç”»åƒãƒ».detect/ä¸¡æ–¹ï¼‰                        â•‘
    â•‘                                                                  â•‘
    â•‘  â— branch_no!=1 (2æšç›®ä»¥é™):                                     â•‘
    â•‘    â†’ ãƒãƒŠãƒ¼è¿½åŠ ã€çµ¶å¯¾ç¦æ­¢ã€‘                                      â•‘
    â•‘    â†’ ãƒã‚¹ã‚¯å‡¦ç†ã®ã¿ï¼ˆ.detect/ã«ä¿å­˜ï¼‰                            â•‘
    â•‘    â†’ å…ƒç”»åƒã¯å¤‰æ›´ã—ãªã„                                          â•‘
    â•‘                                                                  â•‘
    â•‘  â— branch_no=30,31,32 (ç‰¹æ®Šç”»åƒ):                                â•‘
    â•‘    â†’ æ¤œå‡ºã‚¹ã‚­ãƒƒãƒ—ã€ãã®ã¾ã¾.detect/ã«ã‚³ãƒ”ãƒ¼                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    å‡¦ç†ãƒ•ãƒ­ãƒ¼:
    1. .backupãƒ•ã‚©ãƒ«ãƒ€ãŒãªã‘ã‚Œã°ä½œæˆã—ã€å…ƒç”»åƒã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    2. .backupã«æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¸ˆã¿ï¼‰
    3. å‡¦ç†å®Ÿè¡Œï¼ˆ.backupã‹ã‚‰å…¥åŠ›ã—ã¦æ¤œå‡ºç²¾åº¦ã‚’ç¢ºä¿ï¼‰
    4. branch_no=30,31,32ã¯æ¤œå‡ºã‚¹ã‚­ãƒƒãƒ—ã€ã‚³ãƒ”ãƒ¼ã®ã¿

    Args:
        file_path: S3ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: /upfile/1007/4856/20220824190333_1.jpg)
        is_first_image: æœ€åˆã®ç”»åƒã‹ã©ã†ã‹ (True=branch_no=1, ãƒãƒŠãƒ¼è¿½åŠ å¯¾è±¡)
        force_overlay: å¼·åˆ¶çš„ã«ãƒãƒŠãƒ¼ã‚’è¿½åŠ ã™ã‚‹ã‹ï¼ˆbranch_no=1ã®ã¿æœ‰åŠ¹ï¼‰
        force: .detect/ãŒå­˜åœ¨ã—ã¦ã‚‚å¼·åˆ¶çš„ã«å†å‡¦ç†ï¼ˆbranch_no=1ã®ã¿æœ‰åŠ¹ï¼‰
        branch_no: ç”»åƒã®branch_noï¼ˆæ¤œå‡ºã‚¹ã‚­ãƒƒãƒ—åˆ¤å®šç”¨ï¼‰

    Returns:
        dict: å‡¦ç†çµæœ
    """
    # ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    full_path = os.path.join(S3_MOUNT, file_path.lstrip("/"))

    logger.debug(f"å‡¦ç†é–‹å§‹: {full_path}")

    if not os.path.exists(full_path):
        logger.warn(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªæ¤œå‡º: {full_path}")
        return {"status": "skip", "reason": "file_not_found", "path": full_path}

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‘ã‚¹è¨­å®š
    file_name = os.path.basename(full_path)
    relative_path = file_path.lstrip("/")  # upfile/1041/8430/xxx.jpg

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‡¦ç†ï¼ˆ--forceãƒ¢ãƒ¼ãƒ‰ã§ã‚‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒãªã‘ã‚Œã°ä½œæˆï¼‰
    if force:
        # --force ãƒ¢ãƒ¼ãƒ‰: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒãªã‘ã‚Œã°ä½œæˆã€ã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        if BACKUP_S3_BUCKET:
            dir_part = os.path.dirname(relative_path)
            s3_key = f"webroot/{dir_part}/.backup/{file_name}"
            try:
                if not s3_backup_exists(s3_key):
                    logger.debug(
                        f"--force: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ä½œæˆ: s3://{BACKUP_S3_BUCKET}/{s3_key}"
                    )
                    s3_upload_backup(full_path, s3_key)
            except Exception as e:
                logger.warn(f"--force: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {e}")
        elif BACKUP_DIR:
            backup_path = os.path.join(BACKUP_DIR, relative_path)
            if not os.path.exists(backup_path):
                try:
                    backup_dir = os.path.dirname(backup_path)
                    os.makedirs(backup_dir, exist_ok=True)
                    shutil.copy(full_path, backup_path)
                    logger.debug(f"--force: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
                except Exception as e:
                    logger.warn(f"--force: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {e}")
    else:
        # === é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: åˆå›ã®ã¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆå¾©å…ƒã¯æ‰‹å‹•restore_from_backup.pyã§ï¼‰ ===
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
        # å„ªå…ˆé †ä½: BACKUP_S3_BUCKET > BACKUP_DIR
        if BACKUP_S3_BUCKET:
            # === boto3 S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ï¼‰===
            # S3 key: webroot/upfile/1041/8430/.backup/xxx.jpg
            dir_part = os.path.dirname(relative_path)  # upfile/1041/8430
            s3_key = f"webroot/{dir_part}/.backup/{file_name}"

            try:
                backup_exists = s3_backup_exists(s3_key)

                if not backup_exists:
                    # åˆå›: S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                    logger.debug(
                        f"S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: s3://{BACKUP_S3_BUCKET}/{s3_key}"
                    )
                    s3_upload_backup(full_path, s3_key)
                # backup_exists ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆæ‰‹å‹•å¾©å…ƒç”¨ã«ä¿æŒï¼‰
            except Exception as e:
                logger.error(f"S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
                return {
                    "status": "error",
                    "reason": f"s3_backup_failed: {e}",
                    "path": full_path,
                }
        elif BACKUP_DIR:
            # === ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— ===
            backup_path = os.path.join(BACKUP_DIR, relative_path)
            backup_dir = os.path.dirname(backup_path)
            backup_exists = os.path.exists(backup_path)

            if not backup_exists:
                # åˆå›: ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                try:
                    if not os.path.exists(backup_dir):
                        os.makedirs(backup_dir, exist_ok=True)
                    logger.debug(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
                    shutil.copy(full_path, backup_path)
                except Exception as e:
                    logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
                    return {
                        "status": "error",
                        "reason": f"backup_failed: {e}",
                        "path": full_path,
                    }
            # backup_exists ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆæ‰‹å‹•å¾©å…ƒç”¨ã«ä¿æŒï¼‰
        else:
            # ã©ã¡ã‚‰ã‚‚æœªè¨­å®šã¯ã‚¨ãƒ©ãƒ¼
            logger.error("BACKUP_S3_BUCKET ã¾ãŸã¯ BACKUP_DIR ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            return {
                "status": "error",
                "reason": "no_backup_config",
                "path": full_path,
            }

    # .detect/ ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
    dir_part = os.path.dirname(relative_path)  # upfile/1041/8430

    # ============================================================
    # æ¤œå‡ºã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã®branch_no (30, 31, 32)
    # ============================================================
    # ã“ã‚Œã‚‰ã¯è»Šä¸¡ç”»åƒã§ã¯ãªã„ãŸã‚ã€ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆæ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—
    # - 30: ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒ‘ãƒãƒ«
    # - 31: ã‚³ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    # - 32: è»Šæ¤œè¨¼
    # å‡¦ç†: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ â†’ ãã®ã¾ã¾.detect/ã«ã‚³ãƒ”ãƒ¼ï¼ˆæ¤œå‡ºãªã—ï¼‰
    # ============================================================
    skip_detection = branch_no in SKIP_DETECTION_BRANCH_NOS if branch_no else False

    if skip_detection:
        logger.debug(f"æ¤œå‡ºã‚¹ã‚­ãƒƒãƒ—: branch_no={branch_no} (ã‚³ãƒ”ãƒ¼ã®ã¿)")

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆå¿…é ˆï¼‰
        if BACKUP_S3_BUCKET:
            backup_s3_key = f"webroot/{dir_part}/.backup/{file_name}"
            try:
                if not s3_backup_exists(backup_s3_key):
                    s3_upload_backup(full_path, backup_s3_key)
                    logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: s3://{BACKUP_S3_BUCKET}/{backup_s3_key}")
            except Exception as e:
                logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {e}")

            # .detect/ã«ã‚³ãƒ”ãƒ¼ï¼ˆæ¤œå‡ºãªã—ã€ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼ï¼‰
            detect_s3_key = f"webroot/{dir_part}/.detect/{file_name}"
            try:
                s3_upload_backup(full_path, detect_s3_key)
                logger.debug(f".detect/ã‚³ãƒ”ãƒ¼å®Œäº†: s3://{BACKUP_S3_BUCKET}/{detect_s3_key}")
            except Exception as e:
                logger.error(f".detect/ã‚³ãƒ”ãƒ¼å¤±æ•—: {e}")
                return {
                    "status": "error",
                    "reason": f"detect_copy_failed: {e}",
                    "path": full_path,
                }

            return {
                "status": "success",
                "path": full_path,
                "output_path": f"s3://{BACKUP_S3_BUCKET}/{detect_s3_key}",
                "detections": 0,
                "skip_detection": True,
                "branch_no": branch_no,
            }
        elif BACKUP_DIR:
            backup_path = os.path.join(BACKUP_DIR, relative_path)
            if not os.path.exists(backup_path):
                try:
                    backup_dir_path = os.path.dirname(backup_path)
                    os.makedirs(backup_dir_path, exist_ok=True)
                    shutil.copy(full_path, backup_path)
                    logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
                except Exception as e:
                    logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {e}")

            # .detect/ã«ã‚³ãƒ”ãƒ¼ï¼ˆæ¤œå‡ºãªã—ã€ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼ï¼‰
            detect_dir = os.path.join(os.path.dirname(full_path), ".detect")
            detect_output_path = os.path.join(detect_dir, file_name)
            try:
                os.makedirs(detect_dir, exist_ok=True)
                shutil.copy(full_path, detect_output_path)
                logger.debug(f".detect/ã‚³ãƒ”ãƒ¼å®Œäº†: {detect_output_path}")
            except Exception as e:
                logger.error(f".detect/ã‚³ãƒ”ãƒ¼å¤±æ•—: {e}")
                return {
                    "status": "error",
                    "reason": f"detect_copy_failed: {e}",
                    "path": full_path,
                }

            return {
                "status": "success",
                "path": full_path,
                "output_path": detect_output_path,
                "detections": 0,
                "skip_detection": True,
                "branch_no": branch_no,
            }
        else:
            return {
                "status": "error",
                "reason": "no_backup_config",
                "path": full_path,
            }

    # ãƒãƒŠãƒ¼ã®åˆ¤å®šï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰:
    # - First file (branch_no=1) ã®ã¿ãƒãƒŠãƒ¼è¿½åŠ 
    # - --force / --force-overlay ã¯åˆ¥å‡¦ç†
    add_banner_to_detect = is_first_image

    # å‡¦ç†å®Ÿè¡Œï¼ˆTwo-Stage: Seg + Poseï¼‰
    try:
        # ============================================================
        # --force-overlay ãƒ¢ãƒ¼ãƒ‰: å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿ä¸Šæ›¸ã
        # ============================================================
        # å‡¦ç†å†…å®¹:
        #   - branch_no=1 ã®ã¿: å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿ï¼ˆãƒã‚¹ã‚¯ãªã—ï¼‰ã§ä¸Šæ›¸ã
        #   - branch_no!=1: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†ã—ãªã„ï¼‰
        # å‰ææ¡ä»¶:
        #   - .backup ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å…ˆã«ä½œæˆï¼ˆå¿…é ˆï¼‰
        # å…¥åŠ›:
        #   - ç¾åœ¨ã®å…ƒç”»åƒï¼ˆfull_pathï¼‰ã‚’ãã®ã¾ã¾ä½¿ç”¨
        # å‡ºåŠ›:
        #   - å…ƒç”»åƒã‚’ç›´æ¥ä¸Šæ›¸ã
        #   - .detect/ ã¯ä½œæˆã—ãªã„ã€ãƒã‚¹ã‚¯ãªã—
        # ============================================================
        if force_overlay:
            # branch_no=1 ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not is_first_image:
                logger.debug(f"--force-overlay: branch_no!=1 ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                return {
                    "status": "skip",
                    "reason": "force_overlay_not_first",
                    "path": full_path,
                }

            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒãªã‘ã‚Œã°ä½œæˆï¼ˆå¿…é ˆï¼‰
            if BACKUP_S3_BUCKET:
                dir_part = os.path.dirname(relative_path)
                backup_s3_key = f"webroot/{dir_part}/.backup/{file_name}"
                try:
                    if not s3_backup_exists(backup_s3_key):
                        logger.debug(
                            f"--force-overlay: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: s3://{BACKUP_S3_BUCKET}/{backup_s3_key}"
                        )
                        s3_upload_backup(full_path, backup_s3_key)
                except Exception as e:
                    logger.warn(f"--force-overlay: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {e}")
            elif BACKUP_DIR:
                backup_path = os.path.join(BACKUP_DIR, relative_path)
                if not os.path.exists(backup_path):
                    try:
                        backup_dir = os.path.dirname(backup_path)
                        os.makedirs(backup_dir, exist_ok=True)
                        shutil.copy(full_path, backup_path)
                        logger.debug(f"--force-overlay: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
                    except Exception as e:
                        logger.warn(f"--force-overlay: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {e}")

            logger.debug(
                f"--force-overlay: å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿ä¸Šæ›¸ã (masking=False, banner=True)"
            )
            result = process_image(
                input_path=full_path,
                output_path=full_path,
                seg_model=seg_model,
                pose_model=pose_model,
                mask_image=mask_image,
                is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
            )
            result["status"] = "success"
            result["output_path"] = full_path
            result["is_first"] = is_first_image
            result["force_overlay"] = True
            logger.debug(f"--force-overlayå®Œäº†: {full_path}")
            return result

        # ============================================================
        # --force ãƒ¢ãƒ¼ãƒ‰: .detect/ + original ã‚’å¼·åˆ¶å†ä½œæˆ
        # ============================================================
        # å‡¦ç†å†…å®¹:
        #   - branch_no=1:
        #     - .detect/ ã«ãƒã‚¹ã‚¯+ãƒãƒŠãƒ¼ã§ä¸Šæ›¸ã
        #     - å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿ï¼ˆãƒã‚¹ã‚¯ãªã—ï¼‰ã§ä¸Šæ›¸ã
        #   - branch_no!=1:
        #     - .detect/ ã«ãƒã‚¹ã‚¯ã®ã¿ï¼ˆãƒãƒŠãƒ¼ã€çµ¶å¯¾ç¦æ­¢ã€‘ï¼‰ã§ä¸Šæ›¸ã
        #     - å…ƒç”»åƒã¯å¤‰æ›´ã—ãªã„
        # å…¥åŠ›:
        #   - .backup ã‹ã‚‰å…ƒç”»åƒã‚’å–å¾—ï¼ˆæ¤œå‡ºç²¾åº¦ã®ãŸã‚ï¼‰
        #   - .backup ãŒãªã‘ã‚Œã°å…ˆã«ä½œæˆã—ã¦ã‹ã‚‰ä½¿ç”¨
        # å‡ºåŠ›:
        #   - .detect/ ã«å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ä¿å­˜
        #   - branch_no=1: å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã
        # ç”¨é€”:
        #   - é–“é•ã£ã¦ãƒãƒŠãƒ¼ãŒä»˜ã„ãŸ.detect/ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£
        #   - å…ƒç”»åƒã®ãƒãƒŠãƒ¼ã‚’å†é©ç”¨
        # æ³¨æ„:
        #   - .detect/ ã¯å¸¸ã«ãƒã‚¹ã‚¯ã‚ã‚Š
        #   - ãƒãƒŠãƒ¼ã¯ branch_no=1 ã®ã¿
        # ============================================================
        if force:
            # .detect/ ã¯å¸¸ã«ãƒã‚¹ã‚¯ã‚ã‚Š
            # branch_no=1: ãƒã‚¹ã‚¯+ãƒãƒŠãƒ¼
            # branch_no!=1: ãƒã‚¹ã‚¯ã®ã¿ï¼ˆãƒãƒŠãƒ¼ã€çµ¶å¯¾ç¦æ­¢ã€‘ï¼‰
            use_masking = True  # .detect/ ã¯å¸¸ã«ãƒã‚¹ã‚¯ã‚ã‚Š
            use_banner = is_first_image  # branch_no=1 ã®ã¿ãƒãƒŠãƒ¼

            logger.debug(
                f"--force: .detect/å†ä½œæˆ (masking={use_masking}, banner={use_banner})"
            )

            if BACKUP_S3_BUCKET:
                import tempfile

                # .backupã‹ã‚‰å…ƒç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¤œå‡ºç²¾åº¦ã®ãŸã‚ï¼‰
                backup_s3_key = f"webroot/{dir_part}/.backup/{file_name}"
                with tempfile.NamedTemporaryFile(
                    suffix=os.path.splitext(file_name)[1], delete=False
                ) as tmp:
                    temp_input_path = tmp.name

                if s3_backup_exists(backup_s3_key):
                    s3_download_backup(backup_s3_key, temp_input_path)
                    logger.debug(
                        f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›: s3://{BACKUP_S3_BUCKET}/{backup_s3_key}"
                    )
                else:
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒãªã„å ´åˆã¯ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨
                    logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨: {full_path}")
                    shutil.copy(full_path, temp_input_path)

                with tempfile.NamedTemporaryFile(
                    suffix=os.path.splitext(file_name)[1], delete=False
                ) as tmp:
                    temp_detect_path = tmp.name

                result = process_image(
                    input_path=temp_input_path,
                    output_path=temp_detect_path,
                    seg_model=seg_model,
                    pose_model=pose_model,
                    mask_image=mask_image,
                    is_masking=True,  # .detect/ã¯å¸¸ã«ãƒã‚¹ã‚¯ã‚ã‚Š
                    add_banner=use_banner,  # branch_no=1ã®ã¿ãƒãƒŠãƒ¼ã€ãã‚Œä»¥å¤–ã¯çµ¶å¯¾ç¦æ­¢ã€‘
                )

                detect_s3_key = f"webroot/{dir_part}/.detect/{file_name}"
                s3_upload_backup(temp_detect_path, detect_s3_key)
                logger.debug(
                    f".detect/ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{BACKUP_S3_BUCKET}/{detect_s3_key}"
                )

                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆ.detect/å‡¦ç†å®Œäº†ï¼‰
                os.unlink(temp_input_path)
                os.unlink(temp_detect_path)

                # branch_no=1: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ãï¼ˆæœ€å¾Œã«å®Ÿè¡Œï¼‰
                # â€» ç¾åœ¨ã®originalç”»åƒã‚’ä½¿ç”¨ï¼ˆbackupã§ã¯ãªã„ï¼‰
                if is_first_image:
                    logger.debug(
                        f"--force: First file - å…ƒãƒ•ã‚¡ã‚¤ãƒ«(ç¾åœ¨)ã«ãƒãƒŠãƒ¼ã®ã¿ä¸Šæ›¸ã"
                    )
                    with tempfile.NamedTemporaryFile(
                        suffix=os.path.splitext(file_name)[1], delete=False
                    ) as tmp:
                        temp_banner_path = tmp.name

                    # ç¾åœ¨ã®originalç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    original_s3_key = f"webroot/{relative_path}"
                    s3_download_backup(original_s3_key, temp_banner_path)

                    # ãƒãƒŠãƒ¼ã‚’è¿½åŠ ã—ã¦ä¸Šæ›¸ã
                    process_image(
                        input_path=temp_banner_path,
                        output_path=temp_banner_path,
                        seg_model=seg_model,
                        pose_model=pose_model,
                        mask_image=mask_image,
                        is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                        add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
                    )

                    # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã
                    s3_upload_backup(temp_banner_path, original_s3_key)
                    logger.debug(
                        f"å…ƒãƒ•ã‚¡ã‚¤ãƒ«ä¸Šæ›¸ãå®Œäº†: s3://{BACKUP_S3_BUCKET}/{original_s3_key}"
                    )
                    os.unlink(temp_banner_path)
                detect_output_path = f"s3://{BACKUP_S3_BUCKET}/{detect_s3_key}"
                if is_first_image:
                    original_output_path = f"s3://{BACKUP_S3_BUCKET}/{original_s3_key}"
            else:
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›
                backup_path = (
                    os.path.join(BACKUP_DIR, relative_path) if BACKUP_DIR else None
                )
                if backup_path and os.path.exists(backup_path):
                    input_path = backup_path
                    logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›: {backup_path}")
                else:
                    input_path = full_path
                    logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨: {full_path}")

                detect_dir = os.path.join(os.path.dirname(full_path), ".detect")
                detect_output_path = os.path.join(detect_dir, file_name)
                os.makedirs(detect_dir, exist_ok=True)

                result = process_image(
                    input_path=input_path,
                    output_path=detect_output_path,
                    seg_model=seg_model,
                    pose_model=pose_model,
                    mask_image=mask_image,
                    is_masking=True,  # .detect/ã¯å¸¸ã«ãƒã‚¹ã‚¯ã‚ã‚Š
                    add_banner=use_banner,  # branch_no=1ã®ã¿ãƒãƒŠãƒ¼ã€ãã‚Œä»¥å¤–ã¯çµ¶å¯¾ç¦æ­¢ã€‘
                )

                # branch_no=1: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ãï¼ˆæœ€å¾Œã«å®Ÿè¡Œï¼‰
                # â€» ç¾åœ¨ã®originalç”»åƒã‚’ä½¿ç”¨ï¼ˆbackupã§ã¯ãªã„ï¼‰
                if is_first_image:
                    logger.debug(
                        f"--force: First file - å…ƒãƒ•ã‚¡ã‚¤ãƒ«(ç¾åœ¨)ã«ãƒãƒŠãƒ¼ã®ã¿ä¸Šæ›¸ã"
                    )
                    process_image(
                        input_path=full_path,  # ç¾åœ¨ã®originalã‚’ä½¿ç”¨
                        output_path=full_path,
                        seg_model=seg_model,
                        pose_model=pose_model,
                        mask_image=mask_image,
                        is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                        add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
                    )
                    logger.debug(f"å…ƒãƒ•ã‚¡ã‚¤ãƒ«ä¸Šæ›¸ãå®Œäº†: {full_path}")

            result["status"] = "success"
            result["output_path"] = detect_output_path
            result["is_first"] = is_first_image
            result["force"] = True
            if is_first_image:
                result["original_output"] = full_path  # First fileã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°
            logger.debug(f"--forceå®Œäº†: {detect_output_path}")
            return result

        # ============================================================
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æ–°è¦ç”»åƒã‚’å‡¦ç†
        # ============================================================
        # å‡¦ç†å†…å®¹:
        #   - branch_no=1: .detect/ ã«ãƒã‚¹ã‚¯+ãƒãƒŠãƒ¼ã€å…ƒç”»åƒã«ãƒãƒŠãƒ¼ã®ã¿
        #   - branch_no!=1: .detect/ ã«ãƒã‚¹ã‚¯ã®ã¿ï¼ˆãƒãƒŠãƒ¼ãªã—ï¼‰ã€å…ƒç”»åƒã¯å¤‰æ›´ã—ãªã„
        # å…¥åŠ›:
        #   - .backup ã‹ã‚‰å…ƒç”»åƒã‚’å–å¾—ï¼ˆæ¤œå‡ºç²¾åº¦ã®ãŸã‚ï¼‰
        # å‡ºåŠ›:
        #   - .detect/ ã«ãƒã‚¹ã‚¯å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ä¿å­˜
        #   - branch_no=1 ã®ã¿å…ƒç”»åƒã«ãƒãƒŠãƒ¼è¿½åŠ 
        # æ¡ä»¶:
        #   - .detect/ ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        # ============================================================
        # .detect/ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        detect_check_path = os.path.join(
            os.path.dirname(full_path), ".detect", file_name
        )
        if os.path.exists(detect_check_path):
            logger.debug(f".detect/æ—¢å­˜ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {detect_check_path}")
            return {
                "status": "skip",
                "reason": "detect_exists",
                "path": full_path,
                "detect_path": detect_check_path,
            }

        logger.debug(
            f"Two-Stageæ¨è«–é–‹å§‹: .detect/ å‡ºåŠ› (masking=True, banner={add_banner_to_detect})"
        )

        # S3ã®å ´åˆ: tempãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›å¾Œã€boto3ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        # ãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆ: ç›´æ¥.detect/ã«å‡ºåŠ›
        if BACKUP_S3_BUCKET:
            import tempfile

            # .backupã‹ã‚‰å…ƒç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¤œå‡ºç²¾åº¦ã®ãŸã‚ï¼‰
            backup_s3_key = f"webroot/{dir_part}/.backup/{file_name}"
            with tempfile.NamedTemporaryFile(
                suffix=os.path.splitext(file_name)[1], delete=False
            ) as tmp:
                temp_input_path = tmp.name

            if s3_backup_exists(backup_s3_key):
                s3_download_backup(backup_s3_key, temp_input_path)
                logger.debug(
                    f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›: s3://{BACKUP_S3_BUCKET}/{backup_s3_key}"
                )
            else:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒãªã„å ´åˆã¯ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨
                logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨: {full_path}")
                shutil.copy(full_path, temp_input_path)

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
            with tempfile.NamedTemporaryFile(
                suffix=os.path.splitext(file_name)[1], delete=False
            ) as tmp:
                temp_detect_path = tmp.name

            result = process_image(
                input_path=temp_input_path,
                output_path=temp_detect_path,
                seg_model=seg_model,
                pose_model=pose_model,
                mask_image=mask_image,
                is_masking=True,  # ãƒã‚¹ã‚¯ã‚ã‚Š
                add_banner=add_banner_to_detect,  # First fileã®ã¿ãƒãƒŠãƒ¼
            )

            # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            detect_s3_key = f"webroot/{dir_part}/.detect/{file_name}"
            s3_upload_backup(temp_detect_path, detect_s3_key)
            logger.debug(
                f".detect/ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{BACKUP_S3_BUCKET}/{detect_s3_key}"
            )

            # First fileã®ã¿: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã
            # â€» ç¾åœ¨ã®originalç”»åƒã‚’ä½¿ç”¨ï¼ˆ--forceã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            if is_first_image:
                logger.debug(f"First file: å…ƒãƒ•ã‚¡ã‚¤ãƒ«(ç¾åœ¨)ã«ãƒãƒŠãƒ¼ã®ã¿ä¸Šæ›¸ã")
                with tempfile.NamedTemporaryFile(
                    suffix=os.path.splitext(file_name)[1], delete=False
                ) as tmp:
                    temp_banner_path = tmp.name

                # ç¾åœ¨ã®originalç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ--forceã¨åŒã˜ï¼‰
                original_s3_key = f"webroot/{relative_path}"
                s3_download_backup(original_s3_key, temp_banner_path)

                # ãƒãƒŠãƒ¼ã‚’è¿½åŠ 
                process_image(
                    input_path=temp_banner_path,
                    output_path=temp_banner_path,
                    seg_model=seg_model,
                    pose_model=pose_model,
                    mask_image=mask_image,
                    is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                    add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
                )

                # boto3ã§S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                s3_upload_backup(temp_banner_path, original_s3_key)
                logger.debug(f"å…ƒãƒ•ã‚¡ã‚¤ãƒ«ä¸Šæ›¸ãå®Œäº†: s3://{BACKUP_S3_BUCKET}/{original_s3_key}")
                os.unlink(temp_banner_path)

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            os.unlink(temp_input_path)
            os.unlink(temp_detect_path)

            detect_output_path = f"s3://{BACKUP_S3_BUCKET}/{detect_s3_key}"
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›
            backup_path = (
                os.path.join(BACKUP_DIR, relative_path) if BACKUP_DIR else None
            )
            if backup_path and os.path.exists(backup_path):
                input_path = backup_path
                logger.debug(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å…¥åŠ›: {backup_path}")
            else:
                input_path = full_path
                logger.warn(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ã€ç¾åœ¨ã®ç”»åƒã‚’ä½¿ç”¨: {full_path}")

            # ãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆã¯ç›´æ¥å‡ºåŠ›
            detect_dir = os.path.join(os.path.dirname(full_path), ".detect")
            detect_output_path = os.path.join(detect_dir, file_name)
            os.makedirs(detect_dir, exist_ok=True)

            result = process_image(
                input_path=input_path,
                output_path=detect_output_path,
                seg_model=seg_model,
                pose_model=pose_model,
                mask_image=mask_image,
                is_masking=True,  # ãƒã‚¹ã‚¯ã‚ã‚Š
                add_banner=add_banner_to_detect,  # First fileã®ã¿ãƒãƒŠãƒ¼
            )

            # First fileã®ã¿: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒãƒŠãƒ¼ã®ã¿ç‰ˆã‚’ä¸Šæ›¸ã
            # â€» ç¾åœ¨ã®originalç”»åƒã‚’ä½¿ç”¨ï¼ˆ--forceã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            if is_first_image:
                logger.debug(f"First file: å…ƒãƒ•ã‚¡ã‚¤ãƒ«(ç¾åœ¨)ã«ãƒãƒŠãƒ¼ã®ã¿ä¸Šæ›¸ã")
                process_image(
                    input_path=full_path,  # ç¾åœ¨ã®originalç”»åƒ
                    output_path=full_path,
                    seg_model=seg_model,
                    pose_model=pose_model,
                    mask_image=mask_image,
                    is_masking=False,  # ãƒã‚¹ã‚¯ãªã—
                    add_banner=True,  # ãƒãƒŠãƒ¼ã‚ã‚Š
                )

        result["status"] = "success"
        result["output_path"] = detect_output_path
        result["is_first"] = is_first_image
        if is_first_image:
            result["original_output"] = full_path  # First fileã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‘ã‚¹æƒ…å ±
        if BACKUP_S3_BUCKET:
            dir_part = os.path.dirname(relative_path)
            result["backup_path"] = (
                f"s3://{BACKUP_S3_BUCKET}/webroot/{dir_part}/.backup/{file_name}"
            )
        elif BACKUP_DIR:
            result["backup_path"] = os.path.join(BACKUP_DIR, relative_path)

        logger.debug(
            f"å‡¦ç†å®Œäº†: æ¤œå‡ºæ•°={result.get('detections', 0)}, "
            f".detect={detect_output_path}"
            + (f", original={full_path}" if is_first_image else "")
        )
        return result
    except Exception as e:
        logger.error(f"å‡¦ç†å¤±æ•—: {e}")
        return {"status": "error", "reason": str(e), "path": full_path}


# ======================
# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
# ======================
def cleanup_old_logs(log_dir: Path, retention_days: int = 60):
    """å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    if not log_dir.exists():
        return

    cutoff_date = datetime.now() - timedelta(days=retention_days)
    deleted_count = 0

    # process.log.YYYYMMDD å½¢å¼ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
    for file_path in log_dir.glob("process.log.*"):
        try:
            if file_path.stat().st_mtime < cutoff_date.timestamp():
                file_path.unlink()
                deleted_count += 1
                logger.debug(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
        except OSError:
            pass

    # å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ*.logï¼‰
    for file_path in log_dir.glob("*.log"):
        if file_path.name == "process.log":
            continue  # ç¾åœ¨ã®ãƒ­ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—
        try:
            if file_path.stat().st_mtime < cutoff_date.timestamp():
                file_path.unlink()
                deleted_count += 1
                logger.debug(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path.name}")
        except OSError:
            pass

    if deleted_count > 0:
        logger.info(
            f"ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {deleted_count}ä»¶å‰Šé™¤ ({retention_days}æ—¥ä»¥å‰)"
        )


# ======================
# è¨­å®šæ¤œè¨¼
# ======================
def validate_config() -> bool:
    """è¨­å®šã‚’æ¤œè¨¼"""
    errors = []

    if not DB_CONFIG["host"]:
        errors.append("DB_HOST ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not DB_CONFIG["user"]:
        errors.append("DB_USER ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not DB_CONFIG["password"]:
        errors.append("DB_PASSWORD ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not S3_MOUNT:
        errors.append("S3_MOUNT ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    elif not os.path.exists(S3_MOUNT):
        errors.append(f"S3_MOUNT ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {S3_MOUNT}")

    if errors:
        for error in errors:
            logger.error(f"è¨­å®šã‚¨ãƒ©ãƒ¼: {error}")
        logger.error(".env ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return False

    return True


# ======================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ======================
def main():
    # å¼•æ•°ãƒ‘ãƒ¼ã‚¹
    parser = argparse.ArgumentParser(
        description="è»Šä¸¡ç”»åƒã®ãƒŠãƒ³ãƒãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒã‚¹ã‚­ãƒ³ã‚°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python fetch_today_images.py                    # ä»Šæ—¥ã®ç”»åƒã€æœ€å¤§10ä»¶
  python fetch_today_images.py --days-ago 1      # æ˜¨æ—¥ã®ç”»åƒ
  python fetch_today_images.py --date 2026-02-03 # ç‰¹å®šã®æ—¥ä»˜ã‚’æŒ‡å®š
  python fetch_today_images.py --limit 50        # æœ€å¤§50ä»¶å‡¦ç†
  python fetch_today_images.py --days-ago 7 --limit 100
  python fetch_today_images.py --path /1554913G  # ç‰¹å®šãƒ•ã‚©ãƒ«ãƒ€ã‚’ç›´æ¥å‡¦ç†ï¼ˆDBãƒã‚¤ãƒ‘ã‚¹ï¼‰
  python fetch_today_images.py --path 1554913G   # å…ˆé ­/ã¯çœç•¥å¯
        """,
    )
    parser.add_argument(
        "--days-ago",
        type=int,
        default=0,
        help="ä½•æ—¥å‰ã®ç”»åƒã‚’å‡¦ç†ã™ã‚‹ã‹ (0=ä»Šæ—¥, 1=æ˜¨æ—¥, ...) [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0]",
    )
    parser.add_argument(
        "--date",
        type=str,
        default=None,
        help="å‡¦ç†å¯¾è±¡æ—¥ã‚’æŒ‡å®š (YYYY-MM-DDå½¢å¼ã€--days-agoã‚ˆã‚Šå„ªå…ˆ)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=10,
        help="1å›ã®å®Ÿè¡Œã§å‡¦ç†ã™ã‚‹æœ€å¤§è»Šä¸¡æ•° [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10]",
    )
    parser.add_argument(
        "--path",
        type=str,
        default=None,
        help="ç‰¹å®šãƒ•ã‚©ãƒ«ãƒ€ã®ã¿å‡¦ç† (ä¾‹: /1554913G ã¾ãŸã¯ 1554913G) - DBã‚’ãƒã‚¤ãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--force-overlay",
        action="store_true",
        help="å…¨ç”»åƒã«ãƒãƒŠãƒ¼ã‚’å¼·åˆ¶é©ç”¨ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…ˆé ­ç”»åƒã®ã¿)",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help=".detect/ã¨original(branch_no=1)ã‚’å¼·åˆ¶çš„ã«å†å‡¦ç†",
    )
    parser.add_argument(
        "--order",
        type=str,
        choices=["newest", "oldest"],
        default="newest",
        help="å‡¦ç†é †åº: newest=æ–°ã—ã„é †, oldest=å¤ã„é † [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: newest]",
    )

    # å¼•æ•°ã‚’è§£æï¼ˆä¸æ­£ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã‚¨ãƒ©ãƒ¼ï¼‰
    args, unknown = parser.parse_known_args()
    if unknown:
        logger.error(f"ä¸æ­£ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: {unknown}")
        logger.error("æœ‰åŠ¹ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: --days-ago, --date, --limit, --path, --force, --force-overlay, --order")
        return

    # å¯¾è±¡æ—¥ã‚’è¨ˆç®—
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        except ValueError:
            logger.error(f"æ—¥ä»˜å½¢å¼ãŒä¸æ­£: {args.date} (YYYY-MM-DDå½¢å¼ã§æŒ‡å®š)")
            return
    else:
        target_date = datetime.now().date() - timedelta(days=args.days_ago)

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆ
    if BACKUP_S3_BUCKET:
        backup_location = f"s3://{BACKUP_S3_BUCKET}/webroot/.../.backup/ (boto3)"
    elif BACKUP_DIR:
        backup_location = f"{BACKUP_DIR} (ãƒ­ãƒ¼ã‚«ãƒ«)"
    else:
        backup_location = "æœªè¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰"

    order_display = "æ–°ã—ã„é †" if args.order == "newest" else "å¤ã„é †"
    logger.info("=" * 60)
    logger.info(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹ (Two-Stage)")
    if args.path:
        logger.info(f"  å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€: {args.path}")
    else:
        logger.info(f"  å¯¾è±¡æ—¥: {target_date}")
    logger.info(f"  æœ€å¤§å‡¦ç†æ•°: {args.limit}ä»¶")
    logger.info(f"  å‡¦ç†é †åº: {order_display} (--order {args.order})")
    logger.info(f"  S3ãƒã‚¦ãƒ³ãƒˆ: {S3_MOUNT}")
    logger.info(f"  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_location}")
    logger.info(f"  Segãƒ¢ãƒ‡ãƒ«: {SEG_MODEL_PATH}")
    logger.info(f"  Poseãƒ¢ãƒ‡ãƒ«: {POSE_MODEL_PATH}")
    logger.info("=" * 60)

    # è¨­å®šæ¤œè¨¼
    if not validate_config():
        return

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã®ã¿ï¼‰
    try:
        load_models()
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return

    # ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ60æ—¥ä»¥å‰ã‚’å‰Šé™¤ï¼‰
    tracker.cleanup_old_files(LOG_RETENTION_DAYS)
    cleanup_old_logs(LOG_DIR, LOG_RETENTION_DAYS)

    # --path ãƒ¢ãƒ¼ãƒ‰ã‹é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã‹ã§å‡¦ç†ã‚’åˆ†å²
    if args.path:
        # ãƒ•ã‚©ãƒ«ãƒ€ç›´æ¥æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ï¼ˆDBãƒã‚¤ãƒ‘ã‚¹ï¼‰
        logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€ç›´æ¥ãƒ¢ãƒ¼ãƒ‰: {args.path}")
        images = get_images_from_path(args.path)

        if not images:
            logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€ã«ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“: {args.path}")
            return
    else:
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆDBå–å¾—ï¼‰
        # æ—¢å­˜ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆ
        existing_stats = tracker.get_stats(target_date)
        logger.info(
            f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çŠ¶æ³: å‡¦ç†æ¸ˆã¿ {existing_stats['total']}ä»¶ "
            f"(æˆåŠŸ: {existing_stats['success']}, ã‚¨ãƒ©ãƒ¼: {existing_stats['error']})"
        )

        # å¸¸ã«å…¨ä»¶å–å¾—ã—ã€ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã§å‡¦ç†æ¸ˆã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
        # â€» å¢—åˆ†å–å¾—ã¯æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹é€ƒã™ãŸã‚å»ƒæ­¢
        last_fetch_time = None
        logger.info("å…¨ä»¶å–å¾—ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã§å‡¦ç†æ¸ˆã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")

        # ç”»åƒã‚’å–å¾—
        images = get_images_by_date(
            target_date=target_date,
            last_fetch_time=last_fetch_time,
            only_first=args.force_overlay,  # --force-overlayã®å ´åˆã¯branch_no=1ã®ã¿
            order=args.order,  # newest=æ–°ã—ã„é †, oldest=å¤ã„é †
        )

        if not images:
            logger.info(f"{target_date} ã®ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“")
            return

        logger.info(f"DBå–å¾—: {len(images)}ä»¶")

    # è»Šä¸¡ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    car_images = {}
    for row in images:
        (
            file_id,
            car_cd,
            inspresultdata_cd,
            branch_no,
            save_file_name,
            created,
            modified,
        ) = row

        car_key = inspresultdata_cd if inspresultdata_cd else str(car_cd)

        if car_key not in car_images:
            car_images[car_key] = []

        car_images[car_key].append(
            {
                "id": file_id,
                "branch_no": branch_no,
                "path": save_file_name,
                "created": created,
                "modified": modified,
            }
        )

    total_cars = len(car_images)
    cars_to_process = min(total_cars, args.limit)
    logger.info(f"è»Šä¸¡æ•°: {total_cars}å° (å‡¦ç†äºˆå®š: {cars_to_process}å°)")

    # å‡¦ç†ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    stats = {
        "success": 0,
        "skip_tracked": 0,
        "skip_other": 0,
        "error": 0,
        "verified": 0,
        "pending": 0,
    }
    processed_cars = 0  # å‡¦ç†ã—ãŸè»Šä¸¡æ•°ï¼ˆlimitã¯ã“ã‚Œã§åˆ¤å®šï¼‰

    # è»Šä¸¡ã”ã¨ã®çµæœï¼ˆChatworké€šçŸ¥ç”¨ï¼‰
    car_results = []  # [(car_id, success, error, detections), ...]

    # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°
    # --force-overlayã®ã¿ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ.detect/ã‚’ä½œæˆã—ãªã„ãŸã‚ï¼‰
    # --path, --limit, --days-ago, --date ã¯ãƒ•ã‚£ãƒ«ã‚¿ã®ã¿ã€ãƒ­ã‚¸ãƒƒã‚¯ã«å½±éŸ¿ã—ãªã„
    use_tracking = not args.force_overlay

    # å„è»Šä¸¡ã‚’å‡¦ç†
    for car_key, car_files in car_images.items():
        # limitåˆ°é”ãƒã‚§ãƒƒã‚¯ï¼ˆè»Šä¸¡æ•°ã§åˆ¤å®šï¼‰
        if processed_cars >= args.limit:
            logger.info(f"å‡¦ç†ä¸Šé™åˆ°é”: {args.limit}å°")
            break

        # branch_noã§ã‚½ãƒ¼ãƒˆ
        car_files.sort(key=lambda x: x["branch_no"] or 999)

        # ã“ã®è»Šä¸¡ãŒæ—¢ã«ä¸€éƒ¨å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®šï¼‰
        # è»Šä¸¡ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—: /upfile/1041/8430/
        first_file_path = car_files[0]["path"]
        car_dir = os.path.dirname(first_file_path) + "/"  # /upfile/1041/8430/

        # --forceãƒ¢ãƒ¼ãƒ‰ã§ãªã„å ´åˆã€å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒdoneçŠ¶æ…‹ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if use_tracking and not args.force:
            if tracker.has_car_all_done(target_date, car_dir):
                stats["skip_tracked"] += len(car_files)
                logger.debug(f"è»Šä¸¡ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå…¨å®Œäº†ï¼‰: {car_key} (ãƒ•ã‚©ãƒ«ãƒ€: {car_dir})")
                continue

        logger.debug(f"è»Šä¸¡å‡¦ç†é–‹å§‹: {car_key} ({len(car_files)}æš)")

        # è»Šä¸¡ã”ã¨ã®çµ±è¨ˆ
        car_success = 0
        car_error = 0
        car_detections = 0
        car_processed_files = []  # å‡¦ç†æˆåŠŸã—ãŸç”»åƒã®ãƒªã‚¹ãƒˆ [(branch_no, path), ...]
        car_verified_count = 0  # verifiedçŠ¶æ…‹ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°

        # ============================================================
        # Phase 1: å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’pendingçŠ¶æ…‹ã¨ã—ã¦ç™»éŒ²
        # ============================================================
        if use_tracking:
            for file_info in car_files:
                is_first = file_info["branch_no"] == 1
                # æ—¢ã«doneçŠ¶æ…‹ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ--forceã®å ´åˆã¯å†å‡¦ç†ï¼‰
                if not args.force and tracker.is_done(target_date, file_info["id"]):
                    continue
                tracker.mark_pending(
                    target_date=target_date,
                    file_id=file_info["id"],
                    path=file_info["path"],
                    branch_no=file_info["branch_no"],
                    car_id=car_key,
                    is_first=is_first,
                )

        # ============================================================
        # Phase 2: å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        # ============================================================
        for idx, file_info in enumerate(car_files):
            file_id = file_info["id"]

            # branch_no == 1 ã®ã¿ first file ã¨ã—ã¦æ‰±ã†
            is_first = file_info["branch_no"] == 1
            logger.debug(
                f"branch_no={file_info['branch_no']} (type={type(file_info['branch_no']).__name__}), is_first={is_first}"
            )

            # æ—¢ã«doneçŠ¶æ…‹ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ--forceã®å ´åˆã¯å†å‡¦ç†ï¼‰
            if use_tracking and not args.force:
                if tracker.is_done(target_date, file_id):
                    logger.debug(f"ã‚¹ã‚­ãƒƒãƒ—ï¼ˆdoneçŠ¶æ…‹ï¼‰: {file_info['path']}")
                    stats["skip_tracked"] += 1
                    continue

            # processingçŠ¶æ…‹ã«ãƒãƒ¼ã‚¯
            if use_tracking:
                tracker.mark_processing(target_date, file_id)

            # å‡¦ç†å®Ÿè¡Œ
            result = backup_and_process(
                file_path=file_info["path"],
                is_first_image=is_first,
                force_overlay=args.force_overlay,
                force=args.force,
                branch_no=file_info["branch_no"],
            )

            status = result.get("status", "error")

            if status == "success":
                stats["success"] += 1
                car_success += 1
                car_detections += result.get("detections", 0)

                # ============================================================
                # Phase 3: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼
                # ============================================================
                # --force-overlay ã¯ .detect/ ã‚’ä½œæˆã—ãªã„ãŸã‚ã€æ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—
                # skip_detection (branch_no 30,31,32) ã¯ç›´æ¥ã‚³ãƒ”ãƒ¼ã®ãŸã‚ã€æ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—
                if args.force_overlay:
                    car_verified_count += 1
                    stats["verified"] += 1
                    car_processed_files.append(
                        (file_info["branch_no"] or 999, file_info["path"])
                    )
                    logger.success(
                        f"{file_info['path']} "
                        f"(--force-overlay: ãƒãƒŠãƒ¼è¿½åŠ å®Œäº†)"
                    )
                elif result.get("skip_detection"):
                    # skip_detection: ç›´æ¥ã‚³ãƒ”ãƒ¼ã—ãŸã®ã§æ¤œè¨¼ä¸è¦
                    car_verified_count += 1
                    stats["verified"] += 1
                    car_processed_files.append(
                        (file_info["branch_no"] or 999, file_info["path"])
                    )
                    if use_tracking:
                        tracker.mark_verified(
                            target_date=target_date,
                            file_id=file_id,
                            detections=0,
                            output_paths={"detect": result.get("output_path", "")},
                        )
                    logger.success(
                        f"{file_info['path']} "
                        f"(skip_detection: branch_no={result.get('branch_no')}, ã‚³ãƒ”ãƒ¼å®Œäº†)"
                    )
                else:
                    verify_result = verify_output_exists(
                        file_path=file_info["path"],
                        is_first_image=is_first,
                    )

                    if verify_result["verified"]:
                        car_verified_count += 1
                        stats["verified"] += 1

                        # verifiedçŠ¶æ…‹ã«ãƒãƒ¼ã‚¯
                        if use_tracking:
                            output_paths = {
                                "detect": verify_result["detect_path"],
                            }
                            if is_first:
                                output_paths["original"] = verify_result["original_path"]

                            tracker.mark_verified(
                                target_date=target_date,
                                file_id=file_id,
                                detections=result.get("detections", 0),
                                output_paths=output_paths,
                            )

                        # å‡¦ç†æˆåŠŸã—ãŸç”»åƒã‚’è¨˜éŒ²
                        car_processed_files.append(
                            (file_info["branch_no"] or 999, file_info["path"])
                        )

                        logger.success(
                            f"{file_info['path']} "
                            f"(æ¤œå‡º: {result.get('detections', 0)}, "
                            f"ãƒãƒŠãƒ¼: {'ã‚ã‚Š' if is_first else 'ãªã—'}, "
                            f"verified: âœ“)"
                        )
                    else:
                        # å‡¦ç†æˆåŠŸã—ãŸãŒå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ï¼ˆã‚¨ãƒ©ãƒ¼æ‰±ã„ï¼‰
                        stats["error"] += 1
                        car_error += 1
                        if use_tracking:
                            tracker.mark_error(
                                target_date=target_date,
                                file_id=file_id,
                                error_reason=f"output_missing: {verify_result['missing']}",
                            )
                        logger.error(
                            f"{file_info['path']} - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æœªæ¤œå‡º: {verify_result['missing']}"
                        )

            elif status == "skip":
                # --force-overlay ã§ branch_no != 1 ã®å ´åˆãªã©ã‚¹ã‚­ãƒƒãƒ—
                stats["skip_other"] += 1
                logger.debug(
                    f"ã‚¹ã‚­ãƒƒãƒ—: {file_info['path']} - {result.get('reason', 'skip')}"
                )

            elif status == "error":
                stats["error"] += 1
                car_error += 1

                # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã«ãƒãƒ¼ã‚¯
                if use_tracking:
                    tracker.mark_error(
                        target_date=target_date,
                        file_id=file_id,
                        error_reason=result.get("reason", "unknown"),
                    )

                logger.error(f"{file_info['path']} - {result.get('reason', 'unknown')}")

            else:  # skip
                stats["skip_other"] += 1
                logger.debug(
                    f"ã‚¹ã‚­ãƒƒãƒ—: {file_info['path']} - {result.get('reason', '')}"
                )

        # ============================================================
        # Phase 4: è»Šä¸¡ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒverifiedãªã‚‰ã€doneã«æ˜‡æ ¼
        # ============================================================
        if use_tracking and car_verified_count == len(car_files):
            tracker.mark_car_done(target_date, car_dir)
            logger.info(f"è»Šä¸¡å®Œäº†ï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«doneï¼‰: {car_key}")

        # è»Šä¸¡å‡¦ç†å®Œäº†å¾Œã€çµæœã‚’è¨˜éŒ²ï¼ˆå‡¦ç†ãŒã‚ã£ãŸå ´åˆã®ã¿ï¼‰
        if car_success > 0 or car_error > 0:
            processed_cars += 1  # è»Šä¸¡ã‚«ã‚¦ãƒ³ãƒˆ
            logger.info(
                f"[{processed_cars}/{cars_to_process}å°] {car_key}: "
                f"{car_success}æšæˆåŠŸ, {car_error}æšã‚¨ãƒ©ãƒ¼, "
                f"æ¤œå‡º{car_detections}ä»¶, verified: {car_verified_count}/{len(car_files)}"
            )
            # branch_noã§ã‚½ãƒ¼ãƒˆã—ã¦ã‹ã‚‰è¨˜éŒ²
            car_processed_files.sort(key=lambda x: x[0])
            car_results.append(
                (car_key, car_success, car_error, car_detections, car_processed_files)
            )

    # æœ€çµ‚çµ±è¨ˆ
    logger.info("=" * 60)
    logger.info("å‡¦ç†å®Œäº†")
    logger.info(f"  æˆåŠŸ: {stats['success']}ä»¶")
    logger.info(f"  verified: {stats['verified']}ä»¶")
    logger.info(f"  ã‚¨ãƒ©ãƒ¼: {stats['error']}ä»¶")
    logger.info(f"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†æ¸ˆã¿ï¼‰: {stats['skip_tracked']}ä»¶")
    logger.info(f"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãã®ä»–ï¼‰: {stats['skip_other']}ä»¶")

    # --path ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if use_tracking:
        # æœ€çµ‚ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±è¨ˆ
        final_stats = tracker.get_stats(target_date)
        logger.info(
            f"ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç´¯è¨ˆ: {final_stats['total']}ä»¶ "
            f"(pending: {final_stats['pending']}, processing: {final_stats['processing']}, "
            f"verified: {final_stats['verified']}, done: {final_stats['done']}, "
            f"error: {final_stats['error']})"
        )

        # last_fetch_timeã‚’æ›´æ–°ï¼ˆæ¬¡å›ã¯æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾—ï¼‰
        tracker.set_last_processed_time(target_date, datetime.now())
        logger.info(f"æ¬¡å›å¢—åˆ†å–å¾—: {datetime.now().strftime('%H:%M:%S')} ä»¥é™")
    logger.info("=" * 60)

    # Chatworké€šçŸ¥ï¼ˆå‡¦ç†ãŒã‚ã£ãŸå ´åˆã®ã¿ï¼‰
    if CHATWORK_API_KEY and CHATWORK_ROOM_ID and car_results:
        message = build_processing_summary(target_date, stats, car_results)
        if send_chatwork_notification(message):
            logger.info("Chatworké€šçŸ¥é€ä¿¡å®Œäº†")
        else:
            logger.warn("Chatworké€šçŸ¥é€ä¿¡å¤±æ•—")


if __name__ == "__main__":
    main()
